---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 基于线程、进程与异步的爬虫提速

## 提速原理

在进阶篇中，我们详细讲解了线程和进程的特性与使用，它们都是关于功能的**并发**执行，通过并发实现爬虫提速。

而异步编程的原理是关于函数之间的非阻塞执行，我们可以将异步应用于单线程或多线程当中。

多线程是与**具体的执行者**相关的，而异步是与**任务**相关的。

## 多线程与多进程爬虫

原理十分简单，就是多开几条线程，每条进程都会执行爬虫任务。

### 速度对比

下面以向百度(http://www.baidu.com)发起十次`get`请求为例，进行测试：

+ 单线程场景

  ```python
  import requests, time
  
  def test(url):
      res = requests.get(url)
      return res
  
  if __name__ == '__main__':
      start = time.time()
      url = 'http://www.baidu.com'
      for i in range(10):
          test(url)
      res = time.time() - start
      print('单线程耗时：', res)
  ```

  耗时约0.7秒。

+ 多线程场景，每次创建一个线程用于请求：

  ```python
  import requests, time
  from threading import Thread
  
  def test(url,i):
      res = requests.get(url)
      return res
  
  if __name__ == '__main__':
      start = time.time()
      url = 'http://www.baidu.com'
      t = []
      for i in range(10):
          thread = Thread(target=test, args=(url,i))
          t.append(thread)
          thread.start()
      for i in t:			# 等待所有线程执行结束，才可以继续执行
          i.join()		
      res = time.time() - start
      print(res, '多线程')
  ```

  耗时约0.1秒，可见其速度有显著的提升。

### 多线程采集案例

爬取炉石传说官网（https://blizzard.gamespress.com/Hearthstone#?tab=artwork-5 ）中的原画：

+ 刷新后，在网站会更新下面几个包：

  ![image-20220823203238458](https://images.drshw.tech/images/notes/image-20220823203238458.png)

  观察到，图中框出的包中就有我们想要爬取的图片，选择任一个包中的请求网址，访问之。

+ 查看访问后的网站源码发现，图片的地址在网站`a`标签的`href`属性中，使用`XPath`语句`//div/div/a/@href`即可获取：

  ![image-20220823204337633](https://images.drshw.tech/images/notes/image-20220823204337633.png)

  图片的标题也可以用`XPath`语句`//div/div/div/div[@class="thumbDescription"]/text()`进行提取：

  ![image-20220823204939372](https://images.drshw.tech/images/notes/image-20220823204939372.png)

+ 对提取到的`url`进行请求即可：

完整代码参考：

```python
import requests
from lxml import etree
from threading import Thread
import os
import time

start_time = time.time()
# 用于存放线程任务
tasks = []

headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 '
              'Safari/537.36 '
}

# 主程序，用于请求+开启多线程 调用save_image函数
def main(url):
    html = requests.get(url, headers=headers)
    urls = etree.HTML(html.text).xpath('//div[@class="tn jq-tn"]/div/a/@href')
    names = etree.HTML(html.text).xpath('//div/div/div/div[@class="thumbDescription"]/text()')
    if not os.path.exists('hearthstone-images'):
        os.mkdir('hearthstone-images')
    for url, name in zip(urls, names):
        t = Thread(target=save_image, args=(url, name))
        tasks.append(t)
        t.start()

# 用于请求并保存单个图片
def save_image(url, name):
    r = requests.get(url, headers=headers)
    with open('hearthstone-images/' + name + '.png', 'wb') as f:
        f.write(r.content)
        print('保存成功')

if __name__ == '__main__':
    main('https://blizzard.gamespress.com/Files/AjaxThumbnailFetch?Code=4A3A6EF2455B1FD7C890A8A0B4371255'
        '&HideTickboxes=False&Id=12843&width=&maxHeight=&hideExternalFiles=False&includeMetaInfo=True'
        '&addExpiry=True')
    for i in tasks:
        i.join()
    tot_time = time.time() - start_time
    print('全部保存完成，耗时：', tot_time, '秒')
```

### 线程池采集

在进阶篇我们讲解过进程池与进程池任务队列，但是没有提及**线程池**。

事实上，在Python中也提供了线程池。它与进程池的概念相似，也是只不过它不在`threading`模块中，而是在`concurrent.futures.thread`模块中。

可通过模块中的`ThreadPoolExecutor`函数创建一个线程池迭代器，传入一个整数代表线程池中线程的数量。使用线程池时，可使用`with`上下文语法，得到线程对象，调用其`submit()`方法，传入函数名及参数即可启动线程。

相较于进程池，线程池用的很少，下面举一个使用线程池爬取数据的例子（数据源`http://www.xinfadi.com.cn/priceDetail.html` ）：

+ 就发了一个`XHR`包，数据源毫无疑问就是它了：

  ![image-20220823220058843](https://images.drshw.tech/images/notes/image-20220823220058843.png)

+ 发现其页码包含在请求参数的`current`字段中，我们只需要传递对应的请求参数即可实现多页提取：

  ![image-20220823220817699](https://images.drshw.tech/images/notes/image-20220823220817699.png)

开启线程池，使用其中的不同线程对其进行请求即可：

```python
from concurrent.futures.thread import ThreadPoolExecutor
import time
import requests

# 请求并打印每页的请求数据
def download_one_page(data: dict):
    url = 'http://www.xinfadi.com.cn/getPriceData.html'
    # 构造请求头（没有也行 ）
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/92.0.4515.159 Safari/537.36',
        'referer': 'http://www.xinfadi.com.cn/priceDetail.html'
    }

    resp = requests.post(url=url, headers=headers, data=data)
    print(resp.text)

# 主函数，启动请求线程
def download_pages(page_start: int, page_end: int, page_limit: int = 20):
    with ThreadPoolExecutor(100) as t:
        for i in range(page_start, page_end + 1):
            data = {
                'limit': f'{page_limit}',
                'current': f'{i}',
                'pubDateStartTime': '',
                'pubDateEndTime': '',
                'prodPcatid': '',
                'prodCatid': '',
                'prodName': ''
            }
            t.submit(download_one_page, data)

if __name__ == '__main__':
    start_time = time.time()
    # 爬取前100页的信息
    download_pages(page_start=1, page_end=100, page_limit=20)
    end_time = time.time()
    print(f'总耗时{end_time - start_time}s')
```

### 缺点

某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是通行证，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。

因此，Python 中的多线程是不能很好发挥多核优势的，如果想要发挥多核优势，最好还是使用**多进程**。

### 多进程案例

使用多进程，爬取二手车交易网(https://www.2smoto.com/pinpai/)上的机车图片：

+ 文字直接渲染在主页上，通过解析`html`，我们可以轻松地通过`XPath`找到信息存放的节点：

  ![image-20220823222908162](https://images.drshw.tech/images/notes/image-20220823222908162.png)

+ 观察到，页码信息放在了`url`参数中，而最后一页信息在`html`页面中解析获取即可；

+ 开启进程池时，我们将进程数调为与cpu核心数一致，以保证尽可能接近于并行执行。

完整代码参考：

```python
from multiprocessing import Pool
import multiprocessing
from lxml import etree
import requests
import time

# 若x不为空，返回x[0]，若x为空，返回x
maps1 = lambda x: x[0] if x else x

# 对网站进行请求
def request(url):
    # 构造请求头
    headers = {
        'user-agent': '123123',
        'Cookie': 'BAIDU_SSP_lcr=https://www.baidu.com/link?url=MHEtdkDrZiaQ_Fo9zGor7bR9k3gFykSpTtWIpPmJXZvJWVEzlFA6DL83dC7m-1qv&wd=&eqid=ee92cf0700010b5000000006622f2938; Hm_lvt_8b80e9af8bc9476c3b2068990922a408=1647257918; ASPSESSIONIDQWDRCSDC=BLCMBEEDPAPHFAICHNJFGGNA; countsql=%5BS%5Fchexi%5Dwhere+1%3D1; fenyecounts=1183; ASPSESSIONIDQWBTBSDD=BHOENMODOFLOPJIFEHAMEHPC; Hm_lpvt_8b80e9af8bc9476c3b2068990922a408=1647326383',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'
    }
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        res.encoding = 'gb2312'
        parse(res)

# 基础解析模块
def parse_xpath(obj, tag):
    html = etree.HTML(obj)
    text = html.xpath(tag)
    return text

# 解析模块，传入数据，将XPath字符串传给parse_path即可解析
def parse(res):
    # 这里可以使用XPath一步到位地解析，这里解析两次，一次获取ul标签中的所有li标签，再分两次获取有用的信息
    # 纯粹是为了代码简洁
    url = '//ul[@class="goods_list"]/li'
    items = parse_xpath(res.text, url)
    # 获取数据并打印
    for item in items:
        title = maps1(item.xpath('./p[@class="name"]/a/@title'))
        price = maps1(item.xpath('./p[@class="price_wrap"]/span/text()'))
        print({'品牌': title, '价格': price})

# 主函数，开启线程池，调用请求和解析模块
def run():
    url = 'https://www.2smoto.com/pinpai/'
    res = requests.get(url)
    res.encoding = 'gb2312'
    html = maps1(parse_xpath(res.text, "//a[contains(text(),'尾页')]/@href"))
    count = html.split('=')[-1]     # 获取总页数
    # https://www.2smoto.com/pinpai.asp?ppt=&slx=0&skey=&page=2
    # 获取cpu核心数，依据此数量开启进程池
    cpu_count = multiprocessing.cpu_count()
    print("CPU 核心数量是：", cpu_count)
    pool = Pool(processes=cpu_count)
    for i in range(1, int(count) + 1):
        url = 'https://www.2smoto.com/pinpai.asp?ppt=&slx=0&skey=&page={}'.format(i)
        pool.apply_async(request, (url,))
    pool.close()    # 关闭进程池，关闭之后，不能再向进程池中添加进程
    pool.join()     # 当进程池中的所有进程执行完后，主进程才可以继续执行。

if __name__ == '__main__':
    start_time = time.time()
    run()
    print('程序耗时：{}秒'.format(time.time() - start_time))
```

## 异步爬虫

### 概念

我们知道爬虫是 IO 密集型任务，比如如果我们使用 `requests` 库来爬取某个站点的话，发出一个请求之后，程序必须要等待网站返回响应之后才能接着运行，而在等待响应的过程中，整个爬虫程序是一直在等待的，实际上没有做任何的事情。对于这种情况我们有没有优化方案呢？

**异步**为我们解决了这个问题：在异步环境下，进程不必等待请求或其他阻塞执行的代码块执行完毕，即可继续执行后面的代码。

### aiohttp模块

#### 安装

`aiohttp`模块需要进行安装：

```python
pip install aiohttp
```

#### 介绍

`aiohttp` 是一个基于 `asyncio` 的异步 HTTP 网络模块，它既提供了服务端，又提供了客户端。其中我们用服务端可以搭建一个支持异步处理的服务器。

导入该模块后，函数可被`async`修饰，即`async def ...`  ，调用之后将返回一个**协程对象**；

阻塞方法调用时可被`await`修饰，用来**挂起阻塞方法**的执行。

注意，若函数代码段中存在`await`修饰的代码块，必须用`async`修饰该函数。

### 速度对比

下面分别采用同步和异步，对百度官网(https://www.baidu.com)发起100次`get`请求：

+ 同步场景：

  ```python
  import time
  import httpx
  
  def main():
      with httpx.Client() as client:
          for i in range(100):
              res = client.get('https://www.baidu.com')
              print(f'第{i + 1}次请求，status_code = {res.status_code}')
  
  if __name__ == '__main__':
      start = time.time()
      main()
      end = time.time()
      print(f'同步发送100次请求，耗时：{end - start}')
  ```

  耗时约 秒。

+ 异步场景：

  ```python
  import asyncio
  import time
  import httpx
  async def req(client, i):
      res = await client.get('https://www.baidu.com')		# 挂起该方法的执行，不会等待其执行完毕，即可继续执行
      print(f'第{i + 1}次请求，status_code = {res.status_code}')
      return res
  
  async def main():
      async with httpx.AsyncClient() as client:
          task_list = []  # 任务列表
          for i in range(100):
              res = req(client, i)
              task = asyncio.create_task(res)  # 创建任务
              task_list.append(task)
          await asyncio.gather(*task_list)  # 收集任务
  
  if __name__ == '__main__':
      start = time.time()
      asyncio.get_event_loop().run_until_complete(main())
      end = time.time()
      print(f'异步发送100次请求，耗时：{end - start}')
  ```

  



#### **3.3 异步概念**

```python
import asyncio
import time
import httpx
async def req(client, i):
    res = await client.get('https://www.baidu.com')
    print(f'第{i + 1}次请求，status_code = {res.status_code}')
    return res

async def main():
    async with httpx.AsyncClient() as client:
        task_list = []  # 任务列表
        for i in range(100):
            res = req(client, i)
            task = asyncio.create_task(res)  # 创建任务
            task_list.append(task)
        await asyncio.gather(*task_list)  # 收集任务

if __name__ == '__main__':
    start = time.time()
    asyncio.get_event_loop().run_until_complete(main())
    end = time.time()
    print(f'异步发送100次请求，耗时：{end - start}')
```



#### 3.4 任务式采集

***future***：**代表将来执行或还没有执行的任务**

```python
import asyncio
import requests
import time
import aiohttp

start_time = time.time()
urls = [
    'http://127.0.0.1:5000/bobo',
    'http://127.0.0.1:5000/jay',
    'http://127.0.0.1:5000/tom'
]
async def get_page(url):
    async with  aiohttp.ClientSession() as session:
        async with await session.get(url) as response:
        # 这里的session.get/post()的用法跟request.get/post()的用法类似，
        # 唯一的区别是：在使用代理时，要采用该形式：proxy="http://ip:port
            page_text = await response.text()
            #text()返回字符串形式的响应数据
            #read()返回二进制形式的响应数据
            #注意：获取响应数据操作之前一定要使用await进行手动挂起
            print(page_text)


tasks = []
for url in urls:
    c = get_page(url)
    task = asyncio.ensure_future(c)
    tasks.append(task)

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

end_time = time.time()
print("总耗时：", end_time-start_time)

```



#### 3.5 异步案例

采集地址：https://dushu.baidu.com/pc/detail?gid=4295122774

要求：使用协程方式写入，数据存放再mongo 

pip3 install motor

```python
# encoding: utf-8
"""
@author: 夏洛
@QQ: 1972386194
@site: https://www.tulingxueyuan.cn/
@file: 异步携程小说.py
"""

import requests
import asyncio,aiohttp,aiofiles
import pymongo


from motor.motor_asyncio import AsyncIOMotorClient
client = AsyncIOMotorClient('localhost', 27017)
db = client['python']
collection = db['f']

headers = {
    'user-agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
}
b_id= '4295122774'
url = 'https://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"'+b_id +'"}'

import json,os,re

def validateTitle(title):
    # 去除字符串 特殊符号
    rstr = r"[\/\\\:\*\?\"\<\>\|]"  # '/\:*?"<>|'
    new_title = re.sub(rstr, "", title)
    return new_title

async def download(cid,b_id,title):
    data = {
        "book_id":b_id,
        "cid": f"{b_id}|{cid}",
        "need_bookinfo": 1
    }
    data = json.dumps(data)
    url = 'https://dushu.baidu.com/api/pc/getChapterContent?data={}'.format(data)
    async  with aiohttp.ClientSession(headers=headers) as session:
        async  with session.get(url) as resp:
            dic = await resp.json()
            # 异步写入文件
            # print(title)
            content = {
                'title':title,
                'content':dic['data']['novel']['content']
            }
            await save_data(content)

async def save_data(data):
   if data:
       return await collection.insert_one(data)
		

async def getCat(url):
    res = requests.get(url,headers=headers)
    dic = res.json()
    # 提取ID + 章节信息
    tasks = []
    for a in dic['data']['novel']['items']:
        title = a['title']
        cid = a['cid']
        # 准备异步任务
        tasks.append(download(cid,b_id,title))
    await asyncio.wait(tasks)

if __name__ == '__main__':
    # getCat(url)
    asyncio.run(getCat(url))
```

