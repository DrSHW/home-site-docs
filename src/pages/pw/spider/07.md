---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 基于线程、进程与异步的爬虫提速

## 提速原理

在进阶篇中，我们详细讲解了线程和进程的特性与使用，它们都是关于功能的**并发**执行，通过并发实现爬虫提速。

而异步编程的原理是关于函数之间的非阻塞执行，我们可以将异步应用于单线程或多线程当中。

多线程是与**具体的执行者**相关的，而异步是与**任务**相关的。

## 多线程与多进程爬虫

原理十分简单，就是多开几条线程，每条进程都会执行爬虫任务。

### 速度对比

下面以向百度(http://www.baidu.com )发起十次`get`请求为例，进行测试：

+ 单线程场景

  ```python
  import requests, time
  
  def test(url):
      res = requests.get(url)
      return res
  
  if __name__ == '__main__':
      start = time.time()
      url = 'http://www.baidu.com'
      for i in range(10):
          test(url)
      res = time.time() - start
      print('单线程耗时：', res)
  ```

  耗时约0.7秒。

+ 多线程场景，每次创建一个线程用于请求：

  ```python
  import requests, time
  from threading import Thread
  
  def test(url,i):
      res = requests.get(url)
      return res
  
  if __name__ == '__main__':
      start = time.time()
      url = 'http://www.baidu.com'
      t = []
      for i in range(10):
          thread = Thread(target=test, args=(url,i))
          t.append(thread)
          thread.start()
      for i in t:	# 等待所有线程执行结束，才可以继续执行
          i.join()		
      res = time.time() - start
      print(res, '多线程')
  ```

  耗时约0.1秒，可见其速度有显著的提升。

多进程也是一样，就不演示了

### 多线程采集案例

爬取炉石传说官网（https://blizzard.gamespress.com/Hearthstone#?tab=artwork-5 ）中的原画：

+ 刷新后，在网站会更新下面几个包：

  ![image-20220823203238458](https://images.drshw.tech/images/notes/image-20220823203238458.png)

  观察到，图中框出的包中就有我们想要爬取的图片，选择任一个包中的请求网址，访问之。

+ 查看访问后的网站源码发现，图片的地址在网站`a`标签的`href`属性中，使用`XPath`语句`//div/div/a/@href`即可获取：

  ![image-20220823204337633](https://images.drshw.tech/images/notes/image-20220823204337633.png)

  图片的标题也可以用`XPath`语句`//div/div/div/div[@class="thumbDescription"]/text()`进行提取：

  ![image-20220823204939372](https://images.drshw.tech/images/notes/image-20220823204939372.png)

+ 对提取到的`url`进行请求即可：

完整代码参考：

```python
import requests
from lxml import etree
from threading import Thread
import os
import time

start_time = time.time()
# 用于存放线程任务
tasks = []

headers = {
'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 '
              'Safari/537.36 '
}

# 主程序，用于请求+开启多线程 调用save_image函数
def main(url):
    html = requests.get(url, headers=headers)
    urls = etree.HTML(html.text).xpath('//div[@class="tn jq-tn"]/div/a/@href')
    names = etree.HTML(html.text).xpath('//div/div/div/div[@class="thumbDescription"]/text()')
    if not os.path.exists('hearthstone-images'):
        os.mkdir('hearthstone-images')
    for url, name in zip(urls, names):
        t = Thread(target=save_image, args=(url, name))
        tasks.append(t)
        t.start()

# 用于请求并保存单个图片
def save_image(url, name):
    r = requests.get(url, headers=headers)
    with open('hearthstone-images/' + name + '.png', 'wb') as f:
        f.write(r.content)
        print('保存成功')

if __name__ == '__main__':
    main('https://blizzard.gamespress.com/Files/AjaxThumbnailFetch?Code=4A3A6EF2455B1FD7C890A8A0B4371255'
        '&HideTickboxes=False&Id=12843&width=&maxHeight=&hideExternalFiles=False&includeMetaInfo=True'
        '&addExpiry=True')
    for i in tasks:
        i.join()
    tot_time = time.time() - start_time
    print('全部保存完成，耗时：', tot_time, '秒')
```

### 线程池采集

我们在[**池**](https://docs.drshw.tech/pb/senior/10/)一节中介绍过，可通过`concurrent.futures`模块中的`ThreadPoolExecutor`函数创建一个线程池迭代器，传入一个整数代表线程池中线程的数量。使用线程池时，可使用`with`上下文语法，得到线程对象，调用其`submit()`方法，传入函数名及参数即可启动线程。

下面举一个使用线程池爬取数据的例子（数据源`http://www.xinfadi.com.cn/priceDetail.html` ）：

+ 就发了一个`XHR`包，数据源毫无疑问就是它了：

  ![image-20220823220058843](https://images.drshw.tech/images/notes/image-20220823220058843.png)

+ 发现其页码包含在请求参数的`current`字段中，我们只需要传递对应的请求参数即可实现多页提取：

  ![image-20220823220817699](https://images.drshw.tech/images/notes/image-20220823220817699.png)

开启线程池，使用其中的不同线程对其进行请求即可：

```python
from concurrent.futures.thread import ThreadPoolExecutor
import time
import requests

# 请求并打印每页的请求数据
def download_one_page(data: dict):
    url = 'http://www.xinfadi.com.cn/getPriceData.html'
    # 构造请求头（没有也行 ）
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                      'Chrome/92.0.4515.159 Safari/537.36',
        'referer': 'http://www.xinfadi.com.cn/priceDetail.html'
    }

    resp = requests.post(url=url, headers=headers, data=data)
    print(resp.text)

# 主函数，启动请求线程
def download_pages(page_start: int, page_end: int, page_limit: int = 20):
    with ThreadPoolExecutor(100) as t:
        for i in range(page_start, page_end + 1):
            data = {
                'limit': f'{page_limit}',
                'current': f'{i}',
                'pubDateStartTime': '',
                'pubDateEndTime': '',
                'prodPcatid': '',
                'prodCatid': '',
                'prodName': ''
            }
            t.submit(download_one_page, data)

if __name__ == '__main__':
    start_time = time.time()
    # 爬取前100页的信息
    download_pages(page_start=1, page_end=100, page_limit=20)
    end_time = time.time()
    print(f'总耗时{end_time - start_time}s')
```

但是某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是通行证，并且在一个 Python 进程中，GIL 只有一个。拿不到通行证的线程，就不允许执行。这样就会导致，即使是多核条件下，一个 Python 进程下的多个线程，同一时刻也只能执行一个线程。

因此，Python 中的多线程是不能很好发挥多核优势的，如果想要发挥多核优势，最好还是使用**多进程**：

+ 一般会采用进程池`multiprocessing.Pool`或者`concurrent.futures.ProcessPoolExecutor`实现。

### 多进程采集案例

这里使用进程池`Pool`，爬取二手车交易网(https://www.2smoto.com/pinpai/)上的机车图片：

+ 文字直接渲染在主页上，通过解析`html`，我们可以轻松地通过`XPath`找到信息存放的节点：

  ![image-20220823222908162](https://images.drshw.tech/images/notes/image-20220823222908162.png)

+ 观察到，页码信息放在了`url`参数中，而最后一页信息在`html`页面中解析获取即可；

+ 开启进程池时，我们将进程数调为与cpu核心数一致，以保证尽可能接近于并行执行。

完整代码参考：

```python
from multiprocessing import Pool
import multiprocessing
from lxml import etree
import requests
import time

# 若x不为空，返回x[0]，若x为空，返回x
maps1 = lambda x: x[0] if x else x

# 对网站进行请求
def request(url):
    # 构造请求头
    headers = {
        'user-agent': '123123',
        'Cookie': 'BAIDU_SSP_lcr=https://www.baidu.com/link?url=MHEtdkDrZiaQ_Fo9zGor7bR9k3gFykSpTtWIpPmJXZvJWVEzlFA6DL83dC7m-1qv&wd=&eqid=ee92cf0700010b5000000006622f2938; Hm_lvt_8b80e9af8bc9476c3b2068990922a408=1647257918; ASPSESSIONIDQWDRCSDC=BLCMBEEDPAPHFAICHNJFGGNA; countsql=%5BS%5Fchexi%5Dwhere+1%3D1; fenyecounts=1183; ASPSESSIONIDQWBTBSDD=BHOENMODOFLOPJIFEHAMEHPC; Hm_lpvt_8b80e9af8bc9476c3b2068990922a408=1647326383',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'
    }
    res = requests.get(url, headers=headers)
    if res.status_code == 200:
        res.encoding = 'gb2312'
        parse(res)

# 基础解析模块
def parse_xpath(obj, tag):
    html = etree.HTML(obj)
    text = html.xpath(tag)
    return text

# 解析模块，传入数据，将XPath字符串传给parse_path即可解析
def parse(res):
    # 这里可以使用XPath一步到位地解析，这里解析两次，一次获取ul标签中的所有li标签，再分两次获取有用的信息
    # 纯粹是为了代码简洁
    url = '//ul[@class="goods_list"]/li'
    items = parse_xpath(res.text, url)
    # 获取数据并打印
    for item in items:
        title = maps1(item.xpath('./p[@class="name"]/a/@title'))
        price = maps1(item.xpath('./p[@class="price_wrap"]/span/text()'))
        print({'品牌': title, '价格': price})

# 主函数，开启进程池，调用请求和解析模块
def run():
    url = 'https://www.2smoto.com/pinpai/'
    res = requests.get(url)
    res.encoding = 'gb2312'
    html = maps1(parse_xpath(res.text, "//a[contains(text(),'尾页')]/@href"))
    count = html.split('=')[-1]     # 获取总页数
    # https://www.2smoto.com/pinpai.asp?ppt=&slx=0&skey=&page=2
    # 获取cpu核心数，依据此数量开启进程池
    cpu_count = multiprocessing.cpu_count()
    print("CPU 核心数量是：", cpu_count)
    pool = Pool(processes=cpu_count)
    for i in range(1, int(count) + 1):
        url = 'https://www.2smoto.com/pinpai.asp?ppt=&slx=0&skey=&page={}'.format(i)
        pool.apply_async(request, (url,))
    pool.close()    # 关闭进程池，关闭之后，不能再向进程池中添加进程
    pool.join()     # 当进程池中的所有进程执行完后，主进程才可以继续执行。

if __name__ == '__main__':
    start_time = time.time()
    run()
    print('程序耗时：{}秒'.format(time.time() - start_time))
```

## 异步爬虫

### 概念

我们知道爬虫是 IO 密集型任务，比如如果我们使用 `requests` 库来爬取某个站点的话，发出一个请求之后，程序必须要等待网站返回响应之后才能接着运行，而在等待响应的过程中，整个爬虫程序是一直在等待的，实际上没有做任何的事情。这种情况恰好符合了[**异步**](https://docs.drshw.tech/pb/senior/9/)的使用场景。

但是`requests`模块的耗时操作并不支持Python内部的`asyncio`异步，需要使用`asyncio`中事件循环对象`loop`的`run_in_executor()`方法对其进行封装，示例如下：

### 示例和时间对比

下载三张图片，若不使用异步，代码如下：

```python
import requests
import time

def download_image(url):
    print(f'正在下载{url}')
    resource = requests.get(url)
    print(f'{url}下载完成')
    # 图片保存到本地
    file_name = url.split('/')[-1]
    with open(file_name, 'wb') as f:
        f.write(resource.content)
    print(f'{url}下载完成')
    # 图片保存到本地
    file_name = url.split('/')[-1]
    with open(file_name, 'wb') as f:
        f.write(resource.content)


if __name__ == '__main__':
    url_list = [
        'https://img.moegirl.org.cn/common/thumb/e/e9/112049.png/300px-112049.png',
        'https://img.moegirl.org.cn/common/thumb/3/3e/Shikieiki_Yamaxanadu.png/300px-Shikieiki_Yamaxanadu.png',
        'https://img.moegirl.org.cn/common/thumb/6/63/%E5%9B%9B%E5%AD%A3%E6%98%A0%E5%A7%AC%EF%BC%88%E4%BA%BA%E5%A6%96%E5%90%8D%E9%89%B4%EF%BC%89.jpg/330px-%E5%9B%9B%E5%AD%A3%E6%98%A0%E5%A7%AC%EF%BC%88%E4%BA%BA%E5%A6%96%E5%90%8D%E9%89%B4%EF%BC%89.jpg'
    ]
    start = time.time()
    for url in url_list:
        download_image(url)
    end = time.time()
    print(f'总耗时：{end - start}s')
```

使用异步，代码改写如下：

```python
import asyncio
import requests
import time

async def download_image(url):
    # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动地切换到其它任务）
    print(f'正在下载{url}')
    loop = asyncio.get_event_loop()
    # 将耗时操作封装成Future对象
    future = loop.run_in_executor(None, requests.get, url)
    # 遇到阻塞就挂起
    resource = await future
    print(f'{url}下载完成')
    # 图片保存到本地
    file_name = url.split('/')[-1]
    with open(file_name, 'wb') as f:
        f.write(resource.content)

if __name__ == '__main__':
    url_list = [
        'https://img.moegirl.org.cn/common/thumb/e/e9/112049.png/300px-112049.png',
        'https://img.moegirl.org.cn/common/thumb/3/3e/Shikieiki_Yamaxanadu.png/300px-Shikieiki_Yamaxanadu.png',
        'https://img.moegirl.org.cn/common/thumb/6/63/%E5%9B%9B%E5%AD%A3%E6%98%A0%E5%A7%AC%EF%BC%88%E4%BA%BA%E5%A6%96%E5%90%8D%E9%89%B4%EF%BC%89.jpg/330px-%E5%9B%9B%E5%AD%A3%E6%98%A0%E5%A7%AC%EF%BC%88%E4%BA%BA%E5%A6%96%E5%90%8D%E9%89%B4%EF%BC%89.jpg'
    ]
    # 任务列表，封装协程对象
    tasks = [download_image(url) for url in url_list]
    loop = asyncio.get_event_loop()
    start = time.time()
    loop.run_until_complete(asyncio.wait(tasks))
    end = time.time()
    print(f'总耗时：{end - start}s')
```

异步所耗费的时间要短一些。

### `aiohttp` 模块

#### 简介

在使用`requests`模块进行异步请求时，我们总是要对其进行封装，又麻烦又降低效率。

于是，我们有了`aiohttp`模块，它是基于`asyncio`实现的HTTP框架。

官方文档：https://docs.aiohttp.org/en/stable/ 。

它有许多功能，如搭建客户/服务端，我们主要学习它在爬虫方面的应用，即它用作客户端的用法。

#### 准备工作

使用前需要下载：

```python
pip install aiohttp
```

#### 基本使用

直接看一个示例：

使用`aiohttp`异步爬取三个网页：

```python
import aiohttp
import asyncio

async def fetch(session, url):
    print("正在请求：", url)
    async with session.get(url, verify_ssl=False) as response:
        print("请求完成：", url)
        print('状态码:', response.status)
        print('响应头:', response.headers)
        print('响应体:', await response.text())
        print('响应体二进制:', await response.read())
        print('响应体json:', await response.json())

async def main():
    async with aiohttp.ClientSession() as session:
        url_list = [
            'https://www.baidu.com',
            'https://www.sogou.com',
            'https://www.sina.com.cn'
        ]
        tasks = [asyncio.create_task(fetch(session, url)) for url in url_list]
        done, pending = await asyncio.wait(tasks)	# 结果当然也可以用done和pending接收

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())     # 这里直接调用run会报错，总而言之是asyncio.run()会自动关闭循环，算是模块的一个bug
```

其实就是先使用`aiohttp.ClientSession()`函数，实例化出一个`session`对象，通过`session.get(url)`方法对`url`发起请求。

其中：

+ `aiohttp.ClientSession()`可传递`headers`参数，指定请求头，支持`with`上下文语法，`__exit__`方法中定义了连接断开的逻辑；
+ `session.reqType()`可进行请求，这里的`reqType`表示请求类型，如`get`/`post`，可根据请求类型传递`data`/`json`或`params`参数，将返回一个响应对象`resp`，它是一个协程对象，同样支持`with`上下文语法。

#### 超时设置

对于超时的设置，我们可以借助于`ClientTimeout`对象，比如这里我要设置 `1` 秒的超时，可以这么来实现：

```python
import aiohttp
import asyncio

async def main():
    timeout = aiohttp.ClientTimeout(total=1)  # 超时设置，单位秒
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get('https://httpbin.org/get') as response:
            print('status:', response.status)

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

#### 并发限制

由于`aiohttp`可以支持非常大的并发，比如上万、十万、百万都是能做到的，但这么大的并发量，目标网站是很可能在短时间内无法响应的，而且很可能瞬时间将目标网站爬挂掉。所以我们需要控制一下爬取的并发量。

在一般情况下，我们可以借助于`asyncio`的`Semaphore`来控制并发量，代码示例如下：

```python
import asyncio
import aiohttp

CONCURRENCY = 5
URL = 'https://www.baidu.com'
semaphore = asyncio.Semaphore(CONCURRENCY)	# 最大并发量为5
session = None

async def scrape_api():
    async with semaphore:
        print('scraping', URL)
        async with session.get(URL) as response:
            await asyncio.sleep(1)
            return await response.text()

async def main():
    global session
    session = aiohttp.ClientSession()
    scrape_index_tasks = [asyncio.ensure_future(scrape_api()) for _ in range(10000)]
    await asyncio.wait(scrape_index_tasks)

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(main())
```

在这里我们声明了`CONCURRENCY`代表爬取的最大并发量为 `5`，同时声明爬取的目标 URL 为百度。接着我们借助于 `Semaphore` 创建了一个信号量对象，赋值为 `semaphore`，这样我们就可以用它来控制最大并发量了。怎么使用呢？我们这里把它直接放置在对应的爬取方法里面，使用 `async with` 语句将 `semaphore` 作为上下文对象即可。这样的话，信号量可以控制进入爬取的最大协程数量，最大数量就是我们声明的 `CONCURRENCY` 的值。

在 `main` 方法里面，我们声明了 10000 个 `task`，传递给 `gather` 方法运行。倘若不加以限制，这 10000 个 `task` 会被同时执行，并发数量太大。但有了信号量的控制之后，同时运行的 `task` 的数量最大会被控制在 5 个，这样就能给 `aiohttp` 限制速度了。

## 异步数据存储

数据库的连接，执行其实都是耗时操作，可以使用异步优化。

### Mysql异步——`aiomysql`

`aiomysql`是一个用于从`asyncio`框架访问MySQL数据库的模块。它依赖并重用了`pymysql`的大部分部分。

官方文档：https://aiomysql.readthedocs.io/en/latest/ 。

需要先进行安装：

```python
pip install aiomysql
```

使用方式与`pymysql`很接近，只不过多了异步，直接看一个示例：

```python
import asyncio
import aiomysql

async def execute():
    # 连接MySQL
    client = await aiomysql.connect(host='127.0.0.1', port=3306, user='root', password='123456', db='spiders',)
    # 获取游标
    cur = await conn.cursor()
    # 执行SQL
    await cur.execute('select * from news')
    # 获取结果
    result = await cur.fetchall()
    print(result)
    # 关闭游标
    await cur.close()
    # 关闭连接
    client.close()

asyncio.run(execute())
```

连接单个数据库套用即可，在耗时操作前使用`await`任务切换，很好理解。

连接多个数据库时，可以这么做：

```python
import asyncio
import aiomysql

async def execute(host, password, db, sql):
    conn = await aiomysql.connect(host=host, port=3306, user='root', password=password, db=db, charset='utf8')
    cur = await conn.cursor()
    try:
        await cur.execute(sql)	# 执行SQL语句
        await conn.commit()	# 保存操作
    except Exception as e:
        await conn.rollback()	# 回滚
    cur.close()
    conn.close()
    print(f'数据库{host}操作完毕')
    
task_list = [
    # 创建两个任务
    execute('43.12.56.178', '123456', 'spiders', 'insert into news (title, content) values ("title", "content")'),
    execute('43.16.46.128', '123456', 'spiders', 'insert into news (title, content) values ("title", "content")'),
]

asyncio.run(asyncio.wait(task_list))
```

### MongoDB异步——`motor`

`motor`是基于`pymongo`，可以在`asyncio`中使用的异步`mongodb`模块。在`motor.motor-asyncio`中，`motor`使用`ThreadPoolExecutor`将同步阻塞的`pymongo`请求放在多个线程中，通过`callback`回调来达到异步的效果。

官方文档：https://motor.readthedocs.io/en/stable/differences.html 。

需要先进行下载：

```bash
pip install motor
```

可通过`motor.motor_asyncio.AsyncIOMotorClient()`连接`MongoDB`数据库，传入用户名和密码即可，示例：

```python
import motor.motor_asyncio
client = motor.motor_asyncio.AsyncIOMotorClient('localhost', 27017)
```

其余数据操作和`pymongo`中的一致，多了异步而已，这里看一个示例：

```python
import asyncio
import motor.motor_asyncio

async def execute():
    # 连接MongoDB数据库
    client = motor.motor_asyncio.AsyncIOMotorClient('mongodb://localhost:27017', '123456')
    # 选取数据库和集合
    db = client['test']
    collection = db['test']
    # 插入一条数据
    await collection.insert_one({'test': 'test'})
    # 查找一条数据
    print(await collection.find_one())

asyncio.run(execute())
```

连接多个数据库也是一样的，这里不演示了。

## 异步案例

### 案例目标

采集图书：https://dushu.baidu.com/pc/detail?gid=4295122774 的所有章节文本。

要求：使用`aiohttp`异步，数据存放在`MongoDB`中。 

### 案例分析

我们发现，点进任一章节，都会发一个包`getCatalog`：

![image-20221004190006726](C:\Users\17100\AppData\Roaming\Typora\typora-user-images\image-20221004190006726.png)

其中包括了`cid`字段，它会作为`url`中的参数，定位到章节中。我们只需要对其`api`进行请求，拿到`cid`，再异步请求所有`cid`对应的文章地址，并将它们入库即可。

### 代码示例

完整代码如下：

```python
import aiohttp
import asyncio
import requests
from motor.motor_asyncio import AsyncIOMotorClient

client = AsyncIOMotorClient('localhost', 27017)
db = client['python']
collection = db['novels']

headers = {
    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36'
}
b_id = '4295122774'
url = 'https://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"' + b_id + '"}'

import json, os, re

def validateTitle(title):
    # 去除字符串 特殊符号
    rstr = r"[\/\\\:\*\?\"\<\>\|]"  # '/\:*?"<>|'
    new_title = re.sub(rstr, "", title)
    return new_title

async def download(cid, b_id, title):
    data = {
        "book_id": b_id,
        "cid": f"{b_id}|{cid}",
        "need_bookinfo": 1
    }
    data = json.dumps(data)
    url = 'https://dushu.baidu.com/api/pc/getChapterContent?data={}'.format(data)
    async with aiohttp.ClientSession(headers=headers) as session:
        async with session.get(url) as resp:
            dic = await resp.json()
            # 异步写入文件
            # print(title)
            content = {
                'title': title,
                'content': dic['data']['novel']['content']
            }
            await save_data(content)

async def save_data(data):
    if data:
        return await collection.insert_one(data)

async def getCat(url):
    res = requests.get(url, headers=headers)
    dic = res.json()
    # 提取ID + 章节信息
    tasks = []
    for a in dic['data']['novel']['items']:
        title = a['title']
        cid = a['cid']
        # 准备异步任务
        tasks.append(download(cid, b_id, title))
    await asyncio.wait(tasks)

if __name__ == '__main__':
    asyncio.run(getCat(url))
    asyncio.run(getCat(url))
```

