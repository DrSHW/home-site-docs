---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 网络爬虫通讯原理

## 什么是爬虫

简单来讲，爬虫就是一个**探测机器**，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。

爬虫首先要做的工作就是获取网页，即获取网页的**源代码**。源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。

网络爬虫的价值其实就是**数据的价值**，在互联网社会中，数据是无价之宝，一切皆为数据，谁拥有了大量有用的数据，谁就拥有了决策的主动权。

市面上已经有许多**爬虫聚合站点**，即以数据爬虫为核心业务的网站，举几个例子：

+ https://qbt4.mobduos.com/promote/pc/?code=339115928&utm=339115928
+ http://www.hrdatayun.com
+ https://tophub.today/c/tech
+ https://www.vlogxz.com/

## WEB网页基础

网页可以分为三大部分 —— HTML、CSS 和 JavaScript。如果把网页比作一个人的话，HTML 相当于骨架，JavaScript 相当于肌肉，CSS 相当于皮肤，三者结合起来才能形成一个完善的网页。

这部分与前端开发的内容完全重合，传送门：

这一块对爬虫来说很重要（尤其是JavaScript ），建议先了解一些前端的知识再继续学习，这里就不详细讲解了。

## HTTP基本原理

### URI 和 URL

这里我们先了解一下 **URI 和 URL** ：

+ URI 的全称为 Uniform Resource Identifier，即**统一资源标志符**

  举个例子，一个网络地址为：`https://juejin.cn/post/6983626263327932429`：

  按照 URI 的理解：

  1. 这是一个可以通过`https`协议访问的资源，

  2. 位于主机`juejin.cn`上，

  3. 通过`post/6983626263327932429`可以对该资源进行唯一标识（注意，这个不一定是完整的路径 ）

  URI只是一种概念，怎样实现无所谓，只要它**唯一标识一个资源**就可以了。

+ URL 的全称为 Universal Resource Locator，即**统一资源定位符**

  URL 是 URI 概念的一种实现方式。

  通俗地说，URL 是 Internet 上描述信息资源的字符串，主要用在各种`www`客户程序和服务器程序上。

  采用 URL 可以用一种统一的格式来描述各种信息资源，包括文件、服务器的地址和目录等。

  URL 由**协议、IP、端口、资源路径、查询参数、参数值、锚点**组成，格式（方括号内表示可替换成值 ）：

  ````
  [协议]://[IP]:[端口]/[资源路径]/?[查询参数]=[参数值]&[...]#[锚点] 
  ````

  其中，`:[端口]`可省略，`?[查询参数]=[参数值]`可省略，`[查询参数]=[参数值]`可以有很多组，使用`&`进行间隔，`#[锚点]`也可省略。 

  举个例子，对以下 URL 进行分析：

  ```
  https://baike.baidu.com/item/四季映姬·夜摩仙那度?fromtitle=四季映姬&fromid=6529736#1_2
  ```

  协议：`https`

  `IP`：`baike.baidu.com`

  端口字段省略，为`https`协议默认端口`443`（等价于`:443` ）

  资源路径：`item/四季映姬·夜摩仙那度`
  
  查询参数、参数值：`fromtitle=四季映姬&fromid=6529736`
  
  锚点：`1_2`

### 超文本

**超文本**英文名称叫作 hypertext，我们在浏览器里看到的网页就是超文本解析而成的。

其网页源代码是一系列 HTML 代码，里面包含了一系列标签，比如`img`显示图片，`p` 指定显示段落等。

浏览器解析这些标签后，便形成了我们平常看到的网页，而网页的源代码HTML就可以称作超文本。

### HTTP 和 HTTPS

在百度的首页`https://www.baidu.com/`，中，URL 的开头会有`http`或`https`，这个就是访问资源需要的协议类型，有时我们还会看到 `ftp`、`sftp`、`smb` 开头的 URL，那么这里的 `ftp`、`sftp`、`smb` 都是指的协议类型。在爬虫中，我们抓取的页面通常就是 `http` 或 `https` 协议的，我们在这里首先来了解一下这两个协议的含义。

+ HTTP 的全称是 Hyper Text Transfer Protocol，中文名叫做**超文本传输协议**
+ HTTPS 的全称是 Hyper Text Transfer Protocol over Secure Socket Layer，是**以安全为目标的 HTTP 通道**，简单讲是 HTTP 的安全版，即 HTTP 下加入 SSL 层，简称为 HTTPS

#### HTTP 请求过程

我们在浏览器中输入一个 URL，回车之后便会在浏览器中观察到页面内容。

实际上，这个过程是浏览器向网站所在的服务器发送了一个**请求**，网站服务器接收到这个请求后进行处理和解析，然后返回对应的**响应**，接着传回给浏览器。

响应里包含了页面的**源代码**等内容，浏览器再对其进行解析，便将网页呈现了出来。

以`Chrome`浏览器为例，进入`devtools`（F12 ）界面可以看到源代码和网站的模型：

![image-20220813174547584](https://images.maiquer.tech/images/wx/image-20220813174547584.png)

### 请求

请求，由**客户端向服务端**发出，可以分为 4 部分内容：**请求方法**（Request Method ）、**请求的网址**（Request URL ）、**请求头**（Request Headers ）、**请求体**（Request Body ）。

#### 查看请求

打开`devtools`点击上方栏中的网络选项即可看到网站的所有请求面板，结构如下图：

![image-20220813180841445](https://images.maiquer.tech/images/wx/image-20220813180841445.png)

#### 请求方法

一些请求方法名与用法如下表，最常用的为`GET`、`POST`、`PUT`、`DELETE`：

| 方　　法 | 描　　述                                                     |
| -------- | ------------------------------------------------------------ |
| GET      | 请求页面，并返回页面内容                                     |
| HEAD     | 类似于 GET 请求，只不过返回的响应中没有具体的内容，用于获取报头 |
| POST     | 大多用于提交表单或上传文件，数据包含在请求体中               |
| PUT      | 从客户端向服务器传送的数据取代指定文档中的内容               |
| DELETE   | 请求服务器删除指定的页面                                     |
| CONNECT  | 把服务器当作跳板，让服务器代替客户端访问其他网页             |
| OPTIONS  | 允许客户端查看服务器的性能                                   |
| TRACE    | 回显服务器收到的请求，主要用于测试或诊断                     |

#### 请求的网址

请求的网址，即统一资源定位符 URL，它可以唯一确定我们想请求的资源。

#### 请求头

HTTP请求头用于说明是谁或什么在发送请求、请求源于何处，或者客户端的喜好及能力。

服务器可以根据请求头部给出的客户端信息，试着为客户端提供更好的响应。

一些常用的请求头如下表：

| 字　　段          | 描　　述                                                     |
| ----------------- | ------------------------------------------------------------ |
| `Accept`          | 告诉WEB服务器自己接受什么介质类型，`/`表示任何类型，`type/*` 表示该类型下的所有子类型。 |
| `Accept-Language` | 浏览器申明自己接收的语言                                     |
| `Accept-Encoding` | 浏览器申明自己接收的编码方法，通常指压缩方法，是否支持压缩，支持什么压缩方法，如`gzip`, `deflate` |
| `Host`            | 客户端指定自己想访问的WEB服务器的域名/IP地址和端口号         |
| `Cookie`          | 用于维持服务端会话状态，通常由服务端写入，在后续请求中，供服务端读取 |
| `Referer`         | 浏览器向 WEB 服务器表明自己是从哪个 网页/URL 获得/点击 当前请求中的网址/URL |
| `User-Agent`      | 浏览器表明自己的身份（是哪种浏览器 ）                         |
| `Content-Type`    | WEB 服务器告诉浏览器自己响应的对象的类型，如`text/html`代表返回HTML文档，`application/x-javascript`则代表返回 `javascript` 文件，`image/jpeg`则代表返回图片 |
|............||

除此之外，还有很多种请求头字段，可以参考https://byvoid.com/zhs/blog/http-keep-alive-header/。

点击`devtools`中的任意一条请求，即可看到请求的详细信息，其中就包括请求头格式：

![image-20220813193700098](https://images.maiquer.tech/images/wx/image-20220813193700098.png)

请求头不需全部给出，发送请求时只要看服务端规定的请求头格式即可。一般情况下，服务端对请求头的校验不会很严格，即使不传递完整的请求头信息也可完成请求。

#### 请求体

请求体一般承载的内容是 POST 请求中的表单数据，而对于 GET 请求，请求体则为空。

#### 尝试一下

了解了请求的结构后，我们可以试着写我们的第一个爬虫程序了。

我们刚学过基于TCP的`socket`，利用它就可以爬取一个网络页面的源代码。

我们以爬取<a herf='https://www.baidu.com'>百度</a>主页源代码为例，过程可以分为以下三步：

1. 创建一个TCP套接字

2. 连接百度所在的服务器，指定80端口（网站服务端口 ）

3. 通过`HTTP`协议，接收网站的`HTML`源代码

   需要构造一个`HTTP`请求报文，格式如下：

   ![1-2](https://images.maiquer.tech/images/wx/1-2.png)

4. 接收服务器发来的`HTTP`响应报文，将其报文体保存至本地即可

代码如下（这里的代码只要看懂即可 ）

```python
import socket

# 访问网站
url = 'www.baidu.com'
# 绑定端口
port = 80

def TCP_request():
    sock = socket.socket()  # 建立一个TCP套接字（客户端 ）
    sock.connect((url, port))  # 向网站服务端发送连接请求
    request_url = 'GET / HTTP/1.0\r\nHost: www.baidu.com\r\n\r\n'   # 发送HTTP请求报文，按照HTTP报文格式构造
    '''
    参数解释：
        GET: 请求方法，GET请求指的是请求服务器提供的资源，请求的方法是GET
        /: 请求资源的路径，/表示根目录，表示请求服务器的根目录下的资源 
        HTTP/1.0: 请求报文的版本，HTTP/1.0表示请求报文的版本为1.0
        \r\n: 表示请求报文的结束(即回车+换行)，也代表请求报文的结束符
        Host: 请求报文中的主机名
        www.baidu.com: 请求报文中的主机的地址，Host的值
    '''
    sock.send(request_url.encode())     # 将报文发送给服务器
    response = b''                      # 建立一个二进制对象用来存储我们得到的数据
    chunk = sock.recv(1024)             # 接收服务端发送来的回复报文，每次接收的数据不超过1024字节，多次接收
    while chunk:            # 循环接收数据，直到没有数据为止
        response += chunk
        chunk = sock.recv(1024)
    header, html = response.split(b'\r\n\r\n', 1)   # 将回复报文分割为两部分，第一部分是报文头，第二部分是报文体
    print(header.decode('utf-8'))                   # 打印报文头
    f = open('index.html', 'wb')                    # 建立一个二进制文件用来存储报文体
    f.write(html)                                   # 将报文体写入文件
    f.close()                                       # 关闭文件


if __name__ == '__main__':
    TCP_request()
```

 报文体打印结果：

![image-20220813165243612](https://images.maiquer.tech/images/wx/image-20220813165243612.png)

在同级目录下，文件`index.html`也会被创建，其中的内容就是百度主页的`html`源代码。

### 响应

响应，由服务端返回给客户端，可以分为三部分：**响应状态码**（Response Status Code ）、**响应头**（Response Headers ）和**响应体**（Response Body ）。

#### 响应状态码

**响应状态码**表示服务器的响应状态，如 200 代表服务器正常响应，404 代表页面未找到，500 代表服务器内部发生错误。

在爬虫中，我们可以根据状态码来判断服务器响应状态，如状态码为 **200**，则证明**成功**返回数据，再进行进一步的处理，否则直接忽略。

下表列出了常见的状态码及其说明：

| 状态码  | 说　　明           | 详　　情                                                     |
| ------- | ------------------ | ------------------------------------------------------------ |
| 100     | 继续               | 请求者应当继续提出请求。服务器已收到请求的一部分，正在等待其余部分 |
| 101     | 切换协议           | 请求者已要求服务器切换协议，服务器已确认并准备切换           |
| **200** | **成功**           | **服务器已成功处理了请求**                                   |
| 201     | 已创建             | 请求成功并且服务器创建了新的资源                             |
| 202     | 已接受             | 服务器已接受请求，但尚未处理                                 |
| 203     | 非授权信息         | 服务器已成功处理了请求，但返回的信息可能来自另一个源         |
| **204** | **无内容**         | **服务器成功处理了请求，但没有返回任何内容**                 |
| 205     | 重置内容           | 服务器成功处理了请求，内容被重置                             |
| 206     | 部分内容           | 服务器成功处理了部分请求                                     |
| 300     | 多种选择           | 针对请求，服务器可执行多种操作                               |
| 301     | 永久移动           | 请求的网页已永久移动到新位置，即永久重定向                   |
| 302     | 临时移动           | 请求的网页暂时跳转到其他页面，即暂时重定向                   |
| 303     | 查看其他位置       | 如果原来的请求是 POST，重定向目标文档应该通过 GET 提取       |
| 304     | 未修改             | 此次请求返回的网页未修改，继续使用上次的资源                 |
| 305     | 使用代理           | 请求者应该使用代理访问该网页                                 |
| 307     | 临时重定向         | 请求的资源临时从其他位置响应                                 |
| **400** | **错误请求**       | **服务器无法解析该请求**                                     |
| 401     | 未授权             | 请求没有进行身份验证或验证未通过                             |
| **403** | **禁止访问**       | **服务器拒绝此请求**                                         |
| **404** | **未找到**         | **服务器找不到请求的网页**                                   |
| 405     | 方法禁用           | 服务器禁用了请求中指定的方法                                 |
| 406     | 不接受             | 无法使用请求的内容响应请求的网页                             |
| 407     | 需要代理授权       | 请求者需要使用代理授权                                       |
| 408     | 请求超时           | 服务器请求超时                                               |
| 409     | 冲突               | 服务器在完成请求时发生冲突                                   |
| 410     | 已删除             | 请求的资源已永久删除                                         |
| 411     | 需要有效长度       | 服务器不接受不含有效内容长度标头字段的请求                   |
| 412     | 未满足前提条件     | 服务器未满足请求者在请求中设置的其中一个前提条件             |
| 413     | 请求实体过大       | 请求实体过大，超出服务器的处理能力                           |
| 414     | 请求 URI 过长      | 请求网址过长，服务器无法处理                                 |
| 415     | 不支持类型         | 请求格式不被请求页面支持                                     |
| 416     | 请求范围不符       | 页面无法提供请求的范围                                       |
| 417     | 未满足期望值       | 服务器未满足期望请求标头字段的要求                           |
| **500** | **服务器内部错误** | **服务器遇到错误，无法完成请求**                             |
| 501     | 未实现             | 服务器不具备完成请求的功能                                   |
| 502     | 错误网关           | 服务器作为网关或代理，从上游服务器收到无效响应               |
| 503     | 服务不可用         | 服务器目前无法使用                                           |
| 504     | 网关超时           | 服务器作为网关或代理，但是没有及时从上游服务器收到请求       |
| 505     | HTTP 版本不支持    | 服务器不支持请求中所用的 HTTP 协议版本                       |

#### 响应头

响应头包含了服务器对请求的应答信息，如Content-Type、Server、Set-Cookie 等。

一些常用的头信息如下表：

| 字　　段           | 描　　述                                                     |
| ------------------ | ------------------------------------------------------------ |
| `Date`             | 标识响应产生的时间                                           |
| `Last-Modified`    | 指定资源的最后修改时间                                       |
| `Content-Encoding` | 指定响应内容的编码                                           |
| `Server`           | 包含服务器的信息，比如名称、版本号等                         |
| `Content-Type`     | 同请求中的`Content-Type`                                     |
| `Set-Cookie`       | 设置 Cookies。响应头中的 Set-Cookie 告诉浏览器需要将此内容放在 Cookies 中，下次请求携带 Cookies 请求 |
| `Expires`          | 指定响应的过期时间，可以使代理服务器或浏览器将加载的内容更新到缓存中。如果再次访问时，就可以直接从缓存中加载，降低服务器负载，缩短加载时间 |
|..........||

#### 响应体

最重要的当属**响应体**的内容了。响应的**正文数据**都在响应体中，比如请求网页时，它的响应体就是网页的 HTML 代码；请求一张图片时，它的响应体就是图片的二进制数据。我们做爬虫请求网页后，要解析的内容就是响应体。

以对图片的请求为例，网址：https://zh.moegirl.org.cn/%E5%9B%9B%E5%AD%A3%E6%98%A0%E5%A7%AC%C2%B7%E4%BA%9A%E7%8E%9B%E8%90%A8%E9%82%A3%E5%BA%A6

打开`devtools`，点击网络中的`图片`，选择第一个数据包，可显示出请求和响应的信息。点击预览，即可得到响应体的数据：

<img src="https://images.maiquer.tech/images/wx/image-20220813200605034.png" style="zoom:40%;" />

在做爬虫时，我们主要通过响应体得到网页的**源代码、JSON**数据等，然后从中做相应内容的提取。

#### 练练手

提取一张图片，地址为：https://tse2-mm.cn.bing.net/th/id/OIP-C.4xIzIdg9LeKrniEpYBlYlAHaJE?pid=ImgDet&rs=1。

与获取`html`文件一样，我们也要构造并发送一个`http`报文。不同的是，这次返回的数据不仅有响应头，还有响应体。

我们需要使用**正则表达式**提取响应体的内容，并将其存储至文件中。（正则表达式后面会讲，这里的代码只要看懂即可 ）

```python
# 要爬取的资源
url = 'https://tse2-mm.cn.bing.net/th/id/OIP-C.4xIzIdg9LeKrniEpYBlYlAHaJE?pid=ImgDet&rs=1'

import socket
client = socket.socket()        # 建立一个TCP套接字（客户端 ）
client.connect(("tse2-mm.cn.bing.net", 80))     # 向网站服务端发送连接请求

resq = 'GET /th/id/OIP-C.4xIzIdg9LeKrniEpYBlYlAHaJE?pid=ImgDet&rs=1 HTTP/1.1\r\n' \
'Host: tse2-mm.cn.bing.net\r\n\r\n'     # 发送HTTP请求报文

client.send(resq.encode())      # 将报文发送给服务器

result = b''                    # 建立一个二进制对象用来存储我们得到的数据
data = client.recv(1024)
while data:                     # # 接收服务端发送来的回复报文，每次接收的数据不超过1024字节，多次接收
    result += data
    data = client.recv(1024)

import re       # 导入正则表达式模块（后面会讲 ），是用来将响应体中的内容提取出来的，响应体即我们的图片数据
# re.S使 . 匹配包括换行在内的所有字符   去掉响应头
images = re.findall(b'\r\n\r\n(.*)', result, re.S)
# 打开一个文件，将我们读取到的数据存入进去，即下载到本地我们获取到的图片
with open("四季大人.jpg", "wb") as f:
    f.write(images[0])
```

## 会话和Cookies

在浏览网站的过程中，我们经常会遇到需要登录的情况，有些页面只有登录之后才可以访问，而且登录之后可以连续访问很多次网站，但是有时候过一段时间就需要重新登录。

还有一些网站，在打开浏览器时就自动登录了，而且很长时间都不会失效，这种情况又是为什么？

其实这里面涉及会话（Session ）和 Cookies 的相关知识，下面我们就来聊聊它们。

### 无状态 HTTP

在了解会话和 Cookies 之前，我们还需要了解  HTTP 的一个特点，叫作**无状态**，指协议对于交互性场景**没有记忆能力**。

在点击一个纯的`html`网页，请求获取服务器的`html`文件资源时，每次`http`请求都会返回同样的信息，因为这个是没有交互的，每一次的请求都是相互独立的。

第一个请求和第二个请求也没有先后顺序，返回处理哪个，结果都是**同样**的资源页面，因为这种场景是无交互的，无论是什么人请求这个地址，服务器都是返回那个相同的响应。

在面对登陆等场景时，如果返回的都是同样的资源，用户将需要反复登陆验证，这显然是不可接收的。

Cookie和Session的出现解决了这个问题。

### Cookie

Cookie实际上是**一小段的文本信息**。客户端请求服务器，若服务器需要记录该用户状态，就使用响应向客户端浏览器颁发一个Cookie。

客户端**浏览器**会把Cookie保存起来，当浏览器再请求该网站时，**浏览器把请求的网址连同该Cookie一同提交给服务器。**

服务器检查该Cookie，以此来辨认用户状态，也可以根据需要修改Cookie的内容。

Cookie具有不可跨域名性。一个很好理解的例子，在百度上的Cookie内容不能在谷歌上使用，QQ的授权登陆信息不能用于微信。

通过`devtools`可以看到一个网站上的Cookie，以https://www.bing.com为例：

点击上方目录中的应用选项，在右边的存储中，即可找到Cookie。点击即可查看Cookie中的详细字段内容。

![image-20220813220906125](https://images.maiquer.tech/images/wx/image-20220813220906125.png)

其表头字段的含义如下：

| 字　　段    | 描　　述                                                     |
| ----------- | ------------------------------------------------------------ |
| 名称 `Name` | Cookie 的名称。Cookie 一旦创建，名称便不可更改               |
| 值 `Value`  | Cookie 的值。如果值为 Unicode 字符，需要为字符编码。如果值为二进制数据，则需要使用 BASE64 编码 |
| `Max Age`   | Cookie 失效的时间，单位秒，也常和 Expires 一起使用，通过它可以计算出其有效时间。Max Age 如果为正数，则该 Cookie 在 Max Age 秒之后失效。如果为负数，则关闭浏览器时 Cookie 即失效，浏览器也不会以任何形式保存该 Cookie |
| `Path`      | Cookie 的使用路径。如果设置为 `/path/`，则只有路径为 `/path/` 的页面可以访问该 Cookie。如果设置为 `/`，则本域名下的所有页面都可以访问该 Cookie |
| `Domain`    | 可以访问该 Cookie 的域名。若设置字段为`.zhihu.com`，则所有以`zhihu.com`，结尾的域名都可访问该 Cookie |
| `Size`      | 此 Cookie 的大小                                             |
| `Http `     | Cookie 的 `httponly` 属性。若此属性为 `true`，则只有在 HTTP Headers 中会带有此 Cookie 的信息，而不能通过 `document.cookie` 来访问此 Cookie |
| `Secure`    | 该 Cookie 是否仅被使用安全协议传输。安全协议。安全协议有 HTTPS，SSL 等，在网络上传输数据之前先将数据加密。默认为`false` |

了解即可。

### Session 会话

**会话（Session ）**，其本来的含义是指有始有终的一系列动作 / 消息。

由于网页是一种无状态的连接程序，因此无法得知用户的浏览状态。在网上购物的时，把很多商品加入了购物车，而在结账时网站却不知道你购物车有哪些物品。

为了解决这个问题，服务器端就为特定用户创建了特定的Session，用于标示并跟踪这个用户，这样才知道购物车里有哪些商品。

客户端浏览器访问服务器的时候，服务器把客户端信息Session形式记录在服务器上，其要点如下：

- Session是另一种记录客户状态的机制，不同的是Cookie保存在客户端浏览器中，而Session**保存在服务器**上。
- 客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。
- 如果说Cookie机制是通过检查客户身上的**“通行证”**来确定客户身份的话，那么Session机制就是通过检查服务器上的**“客户明细表”**来确认客户身份。
- Session相当于程序在服务器上建立的一份**客户档案**，客户来访的时候只需要查询客户档案表就可以了。

### Session和Cookie的关系

- Cookie 是一个实际存在的、具体的东西，`http`协议中定义在 header 中的字段。
- Session 是一个抽象概念、开发者为了实现中断和继续等操作，将client和server之间一对一的交互，抽象为“会话”，进而衍生出“会话状态”，也就是 Session 的概念。
- 即Session描述的是一种通讯会话机制，而Cookie只是目前实现这种机制的主流方案里面的一个参与者，一般用于保存Session ID。

## 爬虫的基本步骤

爬虫一般分为三个步骤，即**获取网站、提取信息、保存数据**，也可以使用自动化工具（如`selenium`等 ）进行。

### **获取网站**

爬虫首先要做的工作就是**获取网页**，这里就是获取网页的**源代码**。

源代码里包含了网页的部分有用信息，所以只要把源代码获取下来，就可以从中提取想要的信息了。

使用`socket`库进行爬虫过于底层了，用的比较多的有`httpx`和`request`库。

### 提取信息

获取网页源代码后，接下来就是分析网页源代码，从中**提取**我们想要的数据。

首先，最通用的方法便是采用**正则表达式**提取，这是一个万能的方法，但是在构造正则表达式时比较复杂且容易出错。

另外，由于网页的结构有一定的规则，所以还有一些根据网页节点属性、CSS 选择器或 **XPath** 来提取网页信息的库，如 `Beautiful Soup`、`pyquery`、`lxml` 等。使用这些库，我们可以高效快速地从中提取网页信息，如节点的属性、文本值等。

提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得条理清晰，以便我们后续处理和分析数据。

### 保存数据

提取信息后，我们一般会将提取到的数据保存到某处以便后续使用。这里保存形式有多种多样，如可以简单保存为 TXT 文本或 JSON 文本，也可以保存到数据库，如 MySQL 和 MongoDB 等，也可保存至远程服务器，如借助 SFTP 进行操作等。

### 自动化程序

说到自动化程序，意思是说爬虫可以代替人来完成这些操作。

首先，我们手工当然可以提取这些信息，但是当量特别大或者想快速获取大量数据的话，肯定还是要借助程序。

爬虫就是代替我们来完成这份爬取工作的自动化程序，它可以在抓取过程中进行异常处理、错误重试等操作，确保爬取持续高效地运行。



