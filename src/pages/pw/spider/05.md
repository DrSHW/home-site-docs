---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 多种方式数据解析

数据解析，即使用特定**引擎**解析网页中的数据，解析后根据**规则**进行**关键信息的提取**。

常见的技术有：正则、`xpath`、`pyquery`、`bs4`等

## 正则

正则在**任意**文本环境中，按照特定的**规则**对想要的信息进行提取。正则表达式即为规则对应的字符串。

我们可以使用开源测试工具 http://tool.oschina.net/regex/ 对正则表达式进行测试。

官网：https://docs.python.org/zh-cn/3/library/re.html

### 基本语法

正则表达式以`^` 为匹配开始位置，`$`为结束位置，匹配规则在这两个字符之间进行。若要匹配这种特殊字符，需要使用`\`进行转义（`\^`，`\$`）。

#### 正常字符表示

| 普通字符 | 描述                                                         |
| :------- | :----------------------------------------------------------- |
| 单个字符 | 普通字符匹配，按序匹配这些字符，一个单个字符只会用一次。     |
| `[ABC]`  | 匹配 `[...]` 中的所有字符，例如 `[aeiou]` 匹配字符串 `google runoob taobao` 中所有的 e o u a 字母。 |
| `[^ABC]` | 匹配除了 `[...]` 中字符的所有字符，例如 `[^aeiou]` 匹配字符串 `google runoob taobao` 中除了 `e o i u a` 字母的所有字母。 |
| `[A-Z]`  | `[A-Z]` 表示一个区间，匹配所有大写字母，`[a-z]` 表示所有小写字母。 |
| `[0-9]`  | `[A-Z]` 表示一个区间，匹配所有数字。用区间表示的字符，范围均可调整，如`[b-d]`，`[2-4]`。 |
| `.`      | 匹配除`\n`之外的任何单个字符。要匹配包括`\n`在内的任何字符，请使用象`[.\n]`的模式。 |

#### 转义字符表示

| 转义字符 | 描述                                                         |
| -------- | ------------------------------------------------------------ |
| `\d`     | 匹配一个数字字符。等价于 `[0-9]`。                           |
| `\D`     | 匹配一个非数字字符。等价于 `[^0-9]`。                        |
| `\s`     | 匹配任何空白字符，包括空格、制表符、换页符等等。等价于 `[ \f\n\r\t\v]`。 |
| `\S`     | 匹配任何非空白字符。等价于 `[^ \f\n\r\t\v]`。                |
| `\w`     | 匹配包括下划线的任何单词字符。等价于`[A-Za-z0-9]`。          |
| `\W`     | 匹配任何非单词字符。等价于 `[^A-Za-z0-9_]`。                 |
| `\b`     | 匹配一个单词边界，即字与空格间的位置。                       |
| `\B`     | 非单词边界匹配。                                             |

#### 元字符表示

| 字符    | 描述                                                         |
| :------ | :----------------------------------------------------------- |
| `^`     | 匹配输入字符串的**开始**位置。                               |
| `$`     | 匹配输入字符串的**结束**位置。                               |
| `*`     | 匹配前面的子表达式**零次或多次**。例如，`zo*` 能匹配 `"z"`以及 `"zoo"`。 |
| `+`     | 匹配前面的子表达式**一次或多次**。例如，`'zo+'` 能匹配 `"zo"` 以及 `"zoo"`，但不能匹配 `"z"`。 |
| `?`     | 匹配前面的子表达式**零次或一次**。例如，`"do(es)?"` 可以匹配 `"do"` 或`"does"` 。 |
| `{n}`   | `n` 是一个非负整数。匹配**确定**的 `n` 次。例如，`'o{2}'` 不能匹配 `"Bob"` 中的 `'o'`，但是能匹配 `"food"` 中的两个 `o`。 |
| `{n,}`  | `n` 是一个非负整数。**至少**匹配 `n` 次。例如，`'o{2,}'` 不能匹配 `"Bob"` 中的 `'o'`，但能匹配 `"foooood"` 中的所有 `o`。`'o{1,}'` 等价于 `'o+'`。`'o{0,}'` 则等价于 `'o*'`。 |
| `{n,m}` | `m` 和 `n` 均为非负整数，其中`n <= m`。**最少匹配 n 次且最多匹配 m 次**。例如，`"o{1,3}"` 将匹配 `"fooooood"` 中的前三个 `o`。`'o{0,1}'` 等价于 `'o?'`。请注意在逗号和两个数之间**不能有空格**。 |

#### 匹配中文

要想匹配中文，一些编辑器使用`\w`即可做到，也可通过`[\u4e00-\u9fa5]`来表示

许多特殊数据（如身份证号，URL等）的匹配规则都可在 http://tool.oschina.net/regex/ 中找到。

### 匹配模式

正则表达式具有**贪婪模式**和**非贪婪模式**，默认为贪婪模式。

#### 贪婪模式

在贪婪模式下，正则表达式会试图匹配符合规则的，**尽量长**的字符串。

例如：正则表达式`^D.*l$`对`Dustella`的匹配，结果为`Dustell`而不是`Dustel`，原因是默认为贪婪模式，`*`会匹配尽量长的字符串。

#### 非贪婪模式

当`?`字符紧跟在任何一个其他限制符 `*, +, {n}, {n,}, {n,m}` 后面时，匹配模式是**非贪婪**的。

在贪婪模式下，正则表达式会试图匹配符合规则的，**尽量短**的字符串。

还是上面的例子，正则表达式改为`^D.*?l$`，匹配结果就变为`Dustel`，原因是该串为第一个符合正则表达式条件的字符串（局部最短），非贪婪模式中无需再继续向后匹配。

### Python 交互

要想使用正则，需要引入`re`模块，并调用其中的方法。要注意的是，在该模块的正则表达式字符串中，无需添加起始字符`^`和结束字符`$`。

#### match()方法

`match` 方法会尝试从字符串的**起始位置**匹配正则表达式（不会从中间提取），如果匹配，就返回匹配成功的结果；如果不匹配，就返回 `None`。

示例：

```python
import re

content = 'Hello 123 456 my name is DrSHW'
result = re.match('Hello\s\d\d\d\s\d{3}\s\w{2}', content)
print(result)
'''
<re.Match object; span=(0, 16), match='Hello 123 456 my'>
匹配了16个字符，\s等字符的含义与上方的表格相对应。
'''
```

返回结果为一个对象，包含如下方法：

+ `group(num=0)` 属性：尝试匹配的整个表达式的字符串。

  传入`num`参数后，需要在正则表达式中加入一些括号组。在这种情况下，它将返回第`num`组的对应匹配的值；

  示例：

  ```py
  import re
  
  content = 'Hello 123 456 my name is DrSHW'
  result = re.match('Hel(lo\s\d\d)\d\s(\d{3}\s\w{2})', content)
  print(result.group())			# Hello 123 456 my
  print(result.group(2))			# 456 my 
  ```

+ `groups()`方法： 返回一个包含所有小组字符串的**元**组，从 1 到最后所含的小组号。

  在上方代码中添加：

  `print(result.groups())`

  完成测试，预计打印`('lo 12', '456 my')`。

+ `start()`方法：返回匹配开始的位置；

+ `span()`方法：返回一个元组包含匹配 `(开始,结束)` 的位置；

  示例：

  ```python
  ...(接上方代码)
  print(result.start())			# 0
  print(result.span())			# (0, 16) 
  ```

匹配模式示例：

```python
import re

content1 = 'https://dustella.net/posts/Butterfly.html/'
result1 = re.match('http.*net/(.*?)/', content1)
result2 = re.match('http.*net/(.*)/', content1)
# 非贪婪模式
print('result1', result1.group())       # result1 https://dustella.net/posts/
# 贪婪模式
print('result2', result2.group())       # result2 https://dustella.net/posts/Butterfly.html/
```

#### 修饰符

| 标识符 |                                                              |
| ------ | ------------------------------------------------------------ |
| `re.I` | 使匹配对大小写不敏感                                         |
| `re.L` | 做本地化识别（locale-aware）匹配                             |
| `re.M` | 多行匹配，影响 `^` 和 `$`                                    |
| `re.S` | 使 `.` 匹配包括换行在内的所有字符（即支持多行字符串，默认仅支持单行） |
| `re.U` | 根据Unicode字符集解析字符。这个标志影响 `\w`, `\W`, `\b`, `\B`。 |
| `re.X` | 该标志通过给予你更灵活的格式以便你将正则表达式写得更易于理解。 |

一些示例：

```python
import re
# 内容为多行字符串
content1 = '''Hello 1234567 World_This
is a Regex Demo
'''
result1 = re.match('^He.*?(\d+).*?Demo$', content1)
print(result1)              # None
result2 = re.match('^He.*?(\d+).*?Demo$', content1, re.S)    # 使其支持解析多行字符串
print(result2.group())      # ... is a Regex Demo
content2 = 'DrSHW'
result3 = re.match('^drshw$', content2, re.I)    # 使其对大小写不敏感
print(result3.group())      # DrSHW
```

#### search()方法

`re`模块中的`search`函数对**整个字符串**进行从左至右扫描，并返回第一个匹配的结果。

示例：

```python
import re
text = '<ul><li>My name is DrSHW.</li><li>lalala</li></ul>'
res1 = re.search('<li>(.*?)</li>', text)    # 在这种情况下使用match，头部字符就不匹配，将返回None
print(res1.group())
print(res1.group(1))
```

### 正则提取案例

爬取凤凰网财经的源码（地址：https://finance.ifeng.com/c/8HzIujEasuH）并解析其中的**所有数据**：

+ 先引入上节写好的`Spiders`基类，再进行数据请求
+ 查看源码发现，其数据均存放在`js`中的`allData`变量中
+ 构造正则，解析并打印之即可

```python
import re
from utils.base import Spiders
url = 'https://finance.ifeng.com/c/8HzIujEasuH'

res = Spiders().fetch(url)

# 不带符号
text1 = re.findall('var allData = {(.*)};',res.text)
print(text1)
# 带符号
text2 = re.findall('var allData\s=\s(.*);',res.text)
print(text2)
```

## pyquery模块

### 简介

环境安装，指定版本：`pyquery==1.4.3`，即

```bash
pip install pyquery==1.4.3
```

`pyquery`一般用于解析`xml`格式的数据，如常见的`html`等。

它可以获取 DOM 节点的结构，并通过 DOM 节点的一些属性快速进行内容提取。

### 使用前

导入方法：`from pyquery import PyQuery`。

使用前需要先实例化一个处理引擎对象，格式为`doc = PyQuery(要处理的文本字符串)`，通过操作`doc`（名称任意）即可进行数据解析。要注意的是，解析结果依旧是一个`PyQuery`引擎对象，可以对处理过的数据继续进行解析。

### 基本使用

例如，我们对如下字符串进行提取：

```html
html = '''
<div id="cont">
    <ul class="slist">
         <li class="item-0">web开发</li>
         <li class="item-1"><a href="link2.html">爬虫开发</a></li>
         <li class="item-0 active"><a href="link3.html"><span class="bold">数据分析</span></a></li>
         <li class="item-1 active"><a href="link4.html">深度学习</a></li>
         <li class="item-0"><a href="link5.html">机器学习</a></li>
     </ul>
 </div>
'''
```

若想提取所有带`<li>`标签的数据，代码如下：

```python
from pyquery import PyQuery as pq
doc = pq(html)
print(doc('li'))			# 打印所有包含<li>标签的数据
print(type(doc('li')))		 # <class 'pyquery.pyquery.PyQuery'>，返回的还是一个PyQuery对象
```

`pyquery`同样支持根据`CSS`选择器获取节点，示例：

```python
print(doc('#cont .slist li'))
print(type(doc('#cont .slist li')))
```

可以通过`.items()`方法获取一个节点为元素的**生成器**，遍历该生成器即可获取节点信息，通过调用`text()`方法可获取节点数据信息，示例：

```python
for item in doc('#cont .slist li').items():
    print(item)
    print(item.text())
```

`PyQuery`对象的`find()`方法可获取 查询对象符合过滤条件的子节点，传入过滤条件即可，示例：

```python
items = doc('.slist')       # 提取节点所有内容
lis = items.find('li')      # 获取符合条件的li标签
print(lis)
```

通过`PyQuery`对象的`attr()`方法，传入一个属性名字符串，返回该属性的值，示例：

```python
a = doc('.item-0.active a')
print(a)                # <a href="link3.html"><span class="bold">数据分析</span></a>
print(a.attr('href'))   # link3.html
```

遍历提取是常用的提取方式，如果我们需要获取`html`指定节点中所有`href`属性的值，代码示例如下：

```python
a = doc('a')
for s in a.items():
    print(s.attr('href')) 	# 属性获取
    print(s.text())   		# 值获取
```

## xpath

### 简介

`XPath` 的选择功能十分强大，它提供了非常简洁明了的路径选择表达式。另外，它还提供了超过 100 个内建函数，用于字符串、数值、时间的匹配以及节点、序列的处理等。几乎所有我们想要定位的节点，都可以用 `XPath` 来选择。

官网：https://www.w3.org/TR/xpath/ 。 

### 使用前

使用前需要进行环境安装：

```bash
pip install lxml
```

导包方式：

```python
from lxml import etree
```

与`pyquery`相似，`xpath`使用前也需要实例化一个引擎对象，格式为`doc = etree.HTML(要处理的文本字符串)`，用于解析。

### 基本使用

我们一般会用`//`开头的`XPath`规则来选取所有符合要求的节点。`XPath` 常用解析规则如下表：

| 表　达　式         | 描　　述                                             |
| ------------------ | ---------------------------------------------------- |
| `节点名称nodename` | 选取此节点的所有子节点                               |
| `/`                | 从当前节点选取直接子节点                             |
| `//`               | 从当前节点选取子孙节点                               |
| `.`                | 选取当前节点                                         |
| `..`               | 选取当前节点的父节点                                 |
| `@`                | 选取属性，要使用中括号括起，例如：`[@属性名=属性值]` |

我们可以使用浏览器插件`xpath`，更加便捷地协助我们解析数据。

下载地址：https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=zh-CN 。

例如，我们对如下字符串进行提取：

```html
html = '''
<div>
    <ul>
         <li class="item-0"><a href="link1.html">first item</a></li>
         <li class="item-1"><a href="link2.html">second item</a></li>
         <li class="item-inactive"><a href="link3.html">third item</a></li>
         <li class="item-1"><a href="link4.html">fourth item</a></li>
         <li class="item-0"><a href="link5.html">fifth item</a>
     </ul>
 </div>
'''
```

实例化引擎后，我们可以使用`tostring()`方法将引擎对象对应的文本值，序列化为二进制字符串：

```python
from lxml import etree

text = etree.HTML(html)
result = etree.tostring(text)
print(result.decode('utf-8'))
```

如果要选取所有节点，可以这样实现：

```python
result = text.xpath('//*')
```

这里使用 `*`代表匹配所有节点，也就是整个`html`文本中的所有节点都会被获取。

可以看到，返回形式是一个列表，每个元素是`Element`类型，其后跟了节点的名称，如`html`、`body`、`div`、`ul`、`li`、`a` 等，所有节点都包含在列表中了。

若要获取子节点信息，通过`XPath`筛选规则（见上表）就可以做到，将返回由数据信息构成的列表，示例：

```python
result = text.xpath('//li/a')

result = text.xpath('//li/a/text()')  # 提取数据

result = text.xpath('//li/a/@href')   # 属性

result = text.xpath('//li[@class="item-0"]/a/text()')  		# 选取类属性值
```

### 翻页提取

有些网站中信息会分为多页，这时我们就需要进行翻页提取，即获取下一页数据所在的地址再进行爬取。

以网站`http://zb.yfb.qianlima.com/yfbsemsite/mesinfo/zbpglist`为例：

我们需要先定位到`下一页`在`html`中的位置。点击`devtools`上方栏的按钮：

![image-20220821204439240](https://images.drshw.tech/images/notes/image-20220821204439240.png)



找到网页中的下一页按钮，点击之：

![image-20220821204610107](https://images.drshw.tech/images/notes/image-20220821204610107.png)

`devtool`中的一个`a`标签会高亮显示，我们需要找到包含它的盒子：

![image-20220821204836728](https://images.drshw.tech/images/notes/image-20220821204836728.png)

`XPath`定位到节点后，一般情况下其取出`href`路由即可，但由于其`href`属性为`javascript:`（对我们没有帮助），我们需要获取其`onclick`属性，找到后分析其`js`源码即可。对此，`XPath`语句为：

````
//div[@class="pagination"]/ul/li/a[text()="下一页 »"]/@onclick
````

### 案例

使用`XPath`解析器，提取长沙晚报网(https://www.icswb.com/channel-list-channel-162.html)的首页标题信息：

+ 先引入上节写好的`Spiders`基类，再进行数据请求
+ 构造`XPath`字符串，解析请求而来的数据并打印

```python
from lxml import etree
from utils.base import Spiders
url = 'https://www.icswb.com/channel-list-channel-162.html'
res = Spiders().fetch(url=url)
html = etree.HTML(res.text)
li = html.xpath('//ul[@id="NewsListContainer"]/li')
for i in li:
    href = i.xpath('./h3/a/text()')
    print(href)
```

## Beautiful Soup

### 简介

参考文章：https://blog.csdn.net/weixin_55742843/article/details/116535390

简单来说，`BeautifulSoup` 就是 Python 的一个 `html` 或 `xml` 的解析库，我们可以用它来方便地从网页中提取数据，官方的解释如下：

> `BeautifulSoup` 提供一些简单的、Python 式的函数用来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。
>
> `BeautifulSoup` 自动将输入文档转换为 Unicode 编码，输出文档转换为 utf-8 编码。你不需要考虑编码方式，除非文档没有指定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。
>
> `BeautifulSoup` 已成为和 `lxml`、`html5lib` 一样出色的 Python 解释器，为用户灵活地提供不同的解析策略或强劲的速度。

### 使用前

使用前需要进行环境安装：

```python
pip install beautifulsoup4   # bs4
```

导包方式：

```python
from bs4 import BeautifulSoup
```

`BeautifulSoup`使用前也需要实例化一个引擎对象，格式为`soup = BeautifulSoup(要处理的文本字符串, 解析器种类)`。

解析器用于对文本按照特定格式进行解释解析，`BeautifulSoup` 支持的解析器如下表：


| 解析器           | 使用方法                               | 优势                                                        | 劣势                                         |
| ---------------- | -------------------------------------- | ----------------------------------------------------------- | -------------------------------------------- |
| Python 标准库    | `BeautifulSoup(markup, "html.parser")` | Python 的内置标准库、执行速度适中 、文档容错能力强          | Python 2.7.3 or 3.2.2)前的版本中文容错能力差 |
| LXML HTML 解析器 | `BeautifulSoup(markup, "lxml")`        | 速度快、文档容错能力强                                      | 需要安装 C 语言库                            |
| LXML XML 解析器  | `BeautifulSoup(markup, "xml")`         | 速度快、唯一支持 XML 的解析器                               | 需要安装 C 语言库                            |
| html5lib         | `BeautifulSoup(markup, "html5lib")`    | 最好的容错性、以浏览器的方式解析文档、生成 HTML5 格式的文档 | 速度慢、不依赖外部扩展                       |

通过以上对比可以看出，`lxml` 解析器有解析 HTML 和 XML 的功能，而且速度快，容错能力强，所以推荐。

### 基本使用

#### 标签筛选

例如，我们对如下字符串进行提取：

```html
html = '''
<html><head><title>The Dormouse's story</title></head>
<body>
<p class="title" name="dromouse"><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1"><!-- Elsie --></a>,
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
and they lived at the bottom of a well.</p>
<p class="story">...</p>
'''
```

可以通过`soup.标签名`获取**第一个**完整的标签，调用后的类型仍为`BeautifulSoup`对象，即支持嵌套筛选。

通过`soup.标签名.string`获取该标签对应的值(这里`soup`指`BeautifulSoup`对象)。

这种选择方式速度非常快，如果单个节点结构层次非常清晰，可以选用这种方式来解析：

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'lxml')
print(soup.title)       # <title>The Dormouse's story</title>
print(type(soup.title))     # <class 'bs4.element.Tag'>
print(soup.title.string)    # The Dormouse's story
print(soup.head)        # <head><title>The Dormouse's story</title></head>
print(soup.html.head.title.string)	# The Dormouse's story，支持嵌套筛选
print(soup.p)   # <p class="title" name="dromouse"><b>The Dormouse's story</b></p>
print(soup.a)   # <a class="sister" href="http://example.com/elsie" id="link1"><!-- Elsie --></a>
```

每个节点可能有多个属性，比如`id`和`class`等，选择这个节点元素后，可以调用 `attrs` 获取所有属性对应的字典：

```python
print(soup.p.attrs)             # {'class': ['title'], 'name': 'dromouse'}
print(soup.p.attrs['name'])     # dromouse
```

#### 选择器筛选

`BeautifulSoup`同样支持CSS选择器筛选，通过调用其对象的`select`方法实现，传入选择器即可，返回一个元素列表。

我们来看另一个字符串如下：

```html
html = """
<html>
    <head>
        <title>The Dormouse's story</title>
    </head>
    <body>
        <p class="story">
            Once upon a time there were three little sisters; and their names were
            <a href="http://example.com/elsie" class="sister" id="link1" title="xl">
                <span>Elsie</span>
            </a>
            <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> 
            and
            <a href="http://example.com/tillie" class="sister" id="link3" rel="noopener noreferrer ">Tillie</a>
            and they lived at the bottom of a well.
        </p>
        <p class="story">...</p>
"""
```

通过选择器对其进行解析，一些示例：

```python
from bs4 import BeautifulSoup
soup = BeautifulSoup(html, 'lxml')
print(soup.select('.story > a > span')[0].text)     # Elsie
print(soup.select('#link1'))   
print(soup.select('#link1')[0].attrs['href'])   # http://example.com/elsie
print(soup.select("input[type='password']"))    # []
```

### 案例

使用`BeautifulSoup`爬取豆瓣首页（https://movie.douban.com/chart）新片排行榜的电影名：

依旧是先用基类方法请求，再进行解析：

```python
from bs4 import BeautifulSoup
from utils.base import Spiders
url = 'https://movie.douban.com/chart'

resp = Spiders().fetch(url)

soup = BeautifulSoup(resp.text, 'lxml')
res = soup.select('.pl2 a')
for i in res:
    print(i.text)
```

