---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 文本混淆

## 简介

文本混淆简而言之，就是利用前端技术干扰，页面可以正常展示，而使用爬虫下载后无法提取正常的数据。

最常见的混淆方式就是**字体反爬**。

## 字体反爬

### 字体反爬简介

在 `CSS3 `之前，Web 开发者必须使用用户计算机上已有的字体。

`CSS3`的出现允许开发者使用`@font-face`为网页指定字体，开发者可将心仪的字体文件放在 Web 服务器上，并在` CSS` 样式中使用它。用户使用浏览器访问Web应用时，对应的字体会被浏览器下载到用户的计算机上。

### 示例

我们以实习僧详情页（https://www.shixiseng.com/interns?type=intern&city=%E5%85%A8%E5%9B%BD ）这个网站为例。

该网站使用了字体反爬，在详情页中，我们可以看到所有数字和一些汉字都是乱码：

![image-20220910145734630](https://images.drshw.tech/images/notes/image-20220910145734630.png)

显而易见，这种对**静态**资源进行的加密使用自动化工具进行爬取是无效的。

要想破解此类加密，必须搞清楚字体文件编码与正确文件的映射关系，从而进行还原。我们可以使用解析工具进行还原。

## 字体解析工具

### 在线工具

有许多好用的在线字体解析工具，比较推荐的是：http://font.qqe2.com/ 。在其中先对字体有一个大致的概念，再使用本地工具进行提取。

#### 使用方法

还是以刚刚的网站为例，我们先找到它的字体文件。寻找方式也非常简单，在网站源代码中直接查找关键字`font-face`即可（网络抓包也可以 ）：

<img src="https://images.drshw.tech/images/notes/image-20220910151027423.png" alt="image-20220910151027423" style="zoom:50%;" />

`src`中的地址即为字体资源的位置，访问之（根路由+资源位置 ）即可下载它：

下载后，添加后缀`.woff`即可（网站中一般为此格式，有时也会是`.ttf` ）。

下面打开解析工具，将下载的文件导入：

<img src="https://images.drshw.tech/images/notes/image-20220910151700130.png" alt="image-20220910151700130" style="zoom:40%;" />

即可看到所有字符对应的加密串。

下面我们将整个网页使用`request`抓取下来，查找`&#xef64事实习&#xed38`，对应的乱码字符应为`人`和`生`，下方乱码的数字应为`150`和`300`：

<img src="https://images.drshw.tech/images/notes/image-20220910152414928.png" alt="image-20220910152414928" style="zoom:40%;" />

在保存到本地的代码中，我们可以看到：

+ `人`对应的乱码为`&#xef64`，`生`对应`&#xed38`，`1`对应`&#xf717`,`5`对应`&#xed99`，`0`对应`&#xee57`，`3`对应`&#xf673`。

  <img src="https://images.drshw.tech/images/notes/image-20220910153223022.png" alt="image-20220910153223022" style="zoom:40%;" />

与字体解析结果得出的编码完全一致：

![image-20220910153428901](https://images.drshw.tech/images/notes/image-20220910153428901.png)

通过解析结果也可以发现最终转换为的编码为`Unicode`。也就是说，我们使用解析结果，再将其转换为`Unicode`可读字符，直接对网站进行替换还原即可。

### 本地工具

可使用Python的`fontTools`模块进行处理，该模块需要进行下载：

```shell
pip install fontTools
```

#### 基本使用

先引入模块，再通过导入字体文件的方式创建字体文件对象，最后再使用对象输出特定格式的文件即可，以装换为`xml`为例：

```python
from fontTools.ttLib import TTFont
# 加载字体文件：
font = TTFont('file.woff')
# 转为xml文件：
font.saveXML('file.xml')
```

字体文件不仅包含字形数据和点信息，还包括字符到字形映射、字体标题、命名和水平指标等，这些信息存在对应的表中：

| 表     | 作用           |
| ------ | -------------- |
| `cmap` | 字符到字形映射 |
| `glyf` | 字形数据       |
| `head` | 字体标题       |
| `hhea` | 水平标题       |
| `hmtx` | 水平指标       |
| `loca` | 索引到位置     |
| `maxp` | 最大限度的     |
| `name` | 命名           |
| `post` | 后记           |

可以使用`font.keys()`方法查看所有节点：

```python
from fontTools.ttLib import TTFont
# 加载字体文件
font = TTFont('file.woff')
kv = font.keys()
print(kv)
```

使用`font.getBestCmap()`可获取字体编码与名称的映射关系：

```python
code_name_map = font.getBestCmap()
print(code_name_map)	# {120: 'x', 57363: 'uni77', 57554: 'uni42', 57638: 'uni61', 57810: 'uni6e', ...
```

使用`font`对象的`glyf`字段获取字形数据字典，通过`font['glyf'][字体编码].coordinates`获取字体坐标信息：

```python
# 获取请求到的字体形状
glyf = font['glyf']
# font['glyf'][字体编码].coordinates
print(glyf['uni4E94'].coordinates)
```

## 案例实战

### 案例一

#### 逆向目标

+ 爬取实习僧数据页（https://www.shixiseng.com/interns?type=intern&city=%E5%85%A8%E5%9B%BD ）中的静态数据；
+ 需要自动化地绕过字体反爬。

#### 逆向分析

+ 上面已经分析过该网站的字体反爬，也已经在本地下载了对应的字体。下面我们使用Python工具进行破解：

+ 我们介绍了工具的基本使用，下面需要做的就是创建一个`字体编码->Unicode编码`的映射；

+ 分析过，该网站的字体编码格式为：`&#x+一个十六进制数`；而使用本地工具打印时，字体编码为十进制，所以我们先需要将其转为十六进制，再将前缀的`0x`替换为`&#x`：

  ```python
  # 导入过程略
  code_name_map = font.getBestCmap()
  for k, v in code_name_map.items():
      # 将k转换为网页格式
      hex_k = hex(k)
      real_k = hex_k.replace('0x', '&#x')
      print(real_k, v)
  ```

+ 字体文件中的`Unicode`编码的形式为`uni+十六进制数`，而能被网站或程序识别的格式应为`\u+十六进制数`，故也要进行替换（十六进制数需要四位，若不足需要补`0` ）；替换后将其转为可读字符就很容易了，即使用`u'\uxxxx'`即可将`\uxxxx`转成可读字符：

  如此就可以得到创建映射的完整代码：

  ```python
  font_dict = {}
  # 获取请求到的字体形状
  code_name_map = font.getBestCmap()
  for k, v in code_name_map.items():
      hex_k = hex(k)
      real_k = hex_k.replace('0x', '&#x')
      # 将v转为unicode编码
      real_v = v.replace('uni', '\\u') if len(v) == 7 else v.replace('uni', '\\u00')
      # 将unicode编码转为可读字符
      font_dict[real_k] = eval(f'u"{real_v}"')
  print(font_dict)
  ```

+ 最大的问题解决后，即可开始编写爬虫程序，步骤如下：

  1. 获取`html`，且存为`html`文件以便后面使用；
  2. 通过正则定位CSS的`font-face`，下载`html`配套的字体文件；
  3. 提取字体文件信息，并创建映射字典；
  4. 对下载内容（`html`代码 ）进行替换；
  5. 使用`xpath`提取想要的数据

完整代码如下：

```python
import re
from lxml import etree
import requests
from fontTools.ttLib import TTFont

class Spider():
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.96 Safari/537.36"
        }
        self.font_dict = {}

    def get_html(self):
        url = 'https://www.shixiseng.com/interns?type=intern&city=%E5%85%A8%E5%9B%BD'
        ret = requests.get(url=url, headers=self.headers).text
        with open('index.html', 'w', encoding='utf8') as f:
            f.write(ret)
        return ret

    def get_font(self, ret):
        font_url = re.findall('src: url\((.*?)\);', ret)
        f_url = 'https://www.shixiseng.com' + font_url[0] if font_url else font_url
        font_data = requests.get(f_url)
        with open('file.woff', 'wb') as f:
            f.write(font_data.content)

    def get_font_data(self, f_data):
        font = TTFont(f_data)
        code_name_map = font.get("cmap").getBestCmap()
        for k, v in code_name_map.items():
            hex_k = hex(k)
            real_k = hex_k.replace('0x', '&#x')
            # 将v转为unicode编码
            real_v = v.replace('uni', '\\u') if len(v) == 7 else v.replace('uni', '\\u00')
            # 将unicode编码转为可读字符
            self.font_dict[real_k] = eval(f'u"{real_v}"')

    def put_html(self):
        with open("index.html", "r", encoding="utf-8") as f:
            html = f.read()
            for k, v in self.font_dict.items():
                html = html.replace(k, v)
            return html

    def get_data(self, html):
        html = etree.HTML(html)
        li_list = html.xpath("//div[@class='intern-wrap intern-item']")
        for li in li_list:
            title = "".join(li.xpath(".//div[@class='f-l intern-detail__job']//a/text()")[0].split())
            price = "".join(
                li.xpath(".//div[@class='f-l intern-detail__job']//span[@class='day font']/text()")[0].split())
            name = li.xpath('.//a[@class="title ellipsis"]/text()')[0]
            print(title, price, name)

    def main(self):
        # 第1步：获取html，且存为html文件以便后面使用
        ret = self.get_html()
        # 第2步：下载html配套的字体文件
        self.get_font(ret)
        # 第3步：提取字体文件信息，并创建映射字典
        font_dict = self.get_font_data('file.woff')
        # 第4步：对下载内容（html代码 ）进行替换
        html = self.put_html()
        # 第5步：使用xpath提取想要的数据
        data = self.get_data(html)
        print(data)


if __name__ == '__main__':
    Spider().main()

```

### 案例二

#### 逆向目标

+ 爬取人人车数据页（https://www.renrenche.com/nj/ershouche/pr-3-5/?plog_id=a59cba23034e93a5dac80065eadef68f ）中的二手车信息；

+ 打开`devtools`发现两点：

  一是加了Cookie反爬处理，使用了`acw__sc_v2`参数反爬，在上节我们详细介绍过它，这里就不详细讲了。

  二是数据都是静态加载的，但是有些字符被替换成别的了，例如：

  <img src="https://images.drshw.tech/images/notes/image-20220910214025880.png" alt="image-20220910214025880" style="zoom:50%;" />

  使用了字体反爬，需要进行破解。

#### 逆向分析

+ 打开网页源代码搜索`font-face`，找到`woff`字体文件地址：

  <img src="https://images.drshw.tech/images/notes/image-20220910215100725.png" alt="image-20220910215100725" style="zoom:50%;" />

  下载之；

+ 将字体导入在线工具分析，发现字体中只包括数字，且数字与其英文并不匹配：

  ![image-20220910215524988](https://images.drshw.tech/images/notes/image-20220910215524988.png)

  这就很容易了，我们只需要将错误的数字按照文件中的映射关系替换成正确的数字即可。

+ 请求字体链接，获取字体`code`和`name`的对应关系，然后遍历修正偏移量，获取网页中反爬文字的真实文字。

  该过程的完整代码如下：

  ```python
  from io import BytesIO
  import requests
  from fontTools.ttLib import TTFont
  
  font_url = 'https://misc.rrcimg.com/ttf/rrcttfcd3361a897aa43b3f7b785ca37ea0f3b.woff'
  
  def woff_font(font_url):
      '''获取字体真实对应关系'''
      newmap = {}
      resp = requests.get(font_url)  # 请求字体链接
      woff_data = BytesIO(resp.content)  # 保存字体数据
      font = TTFont(woff_data)  # 读取woff数据
      glyf = font['glyf']  # 获取请求到的字体形状
      # 建立基础的字体和字体形状的对应关系
      base_font_map = {
          0: glyf['zero'],
          1: glyf['four'],
          2: glyf['five'],
          3: glyf['three'],
          4: glyf['seven'],
          5: glyf['one'],
          6: glyf['two'],
          7: glyf['six'],
          8: glyf['nine'],
          9: glyf['eight']
      }
      code_name_map = font.getBestCmap()  # 获取请求到的字体code和name的对应关系
      font.close()
      for code, name in code_name_map.items():
          codestr = str(code - 48)  # 根据分析结果需要减去48，即减去偏移量
          print(code, name)
          current_shape = glyf[name]  # 根据name获取字体形状
          for number, shape in base_font_map.items():  # 遍历基础字体形状对应关系
              if shape == current_shape:  # 判断，如果两个字体形状相等
                  newmap[codestr] = str(number)  # 将字体编码和字体添加到字典
      return newmap
  
  print(woff_font(font_url))
  ```

带Cookie反爬的完整代码如下：

JS代码：

```js
// demo.js
function get_cookie(arg1){
    var _0x5e8b26 = '3000176000856006061501533003690027800375'  // 基本上不变
    String['prototype']['unsbox'] = function() {
            var _0x4b082b = [0xf, 0x23, 0x1d, 0x18, 0x21, 0x10, 0x1, 0x26, 0xa, 0x9, 0x13, 0x1f, 0x28, 0x1b, 0x16, 0x17, 0x19, 0xd, 0x6, 0xb, 0x27, 0x12, 0x14, 0x8, 0xe, 0x15, 0x20, 0x1a, 0x2, 0x1e, 0x7, 0x4, 0x11, 0x5, 0x3, 0x1c, 0x22, 0x25, 0xc, 0x24];
        var _0x4da0dc = [];
        var _0x12605e = '';
        for (var _0x20a7bf = 0x0; _0x20a7bf < _0x4b082b.length; _0x20a7bf++) {
            var _0x385ee3 = this[_0x20a7bf];
            for (var _0x217721 = 0x0; _0x217721 < _0x4b082b['length']; _0x217721++) {
                if (_0x4b082b[_0x217721] == _0x20a7bf + 0x1) {
                    _0x4da0dc[_0x217721] = _0x385ee3;
                }
            }
        }
        _0x12605e = _0x4da0dc['\x6a\x6f\x69\x6e']('');
        return _0x12605e;
    }
    String['prototype']['hexXor'] = function(_0x4e08d8) {
        var _0x5a5d3b = '';
        for (var _0xe89588 = 0x0; _0xe89588 < this['length'] && _0xe89588 < _0x4e08d8['length']; _0xe89588 += 0x2) {
            var _0x401af1 = parseInt(this['slice'](_0xe89588, _0xe89588 + 0x2), 0x10);
            var _0x105f59 = parseInt(_0x4e08d8['slice'](_0xe89588, _0xe89588 + 0x2), 0x10);
            var _0x189e2c = (_0x401af1 ^ _0x105f59)['toString'](0x10);
            if (_0x189e2c['length'] == 0x1) {
                _0x189e2c = '\x30' + _0x189e2c;
            }
            _0x5a5d3b += _0x189e2c;
        }
        return _0x5a5d3b;
    }
    var _0x23a392 = arg1['unsbox']();
    arg2 = _0x23a392['hexXor'](_0x5e8b26);  // 需要验证的
    acw_sc__v2 = arg2
    return acw_sc__v2
   // console.log(acw_sc__v2);
}
```

Python代码：

```python
# demo.py
from io import BytesIO
import requests
from fontTools.ttLib import TTFont
from lxml import etree
import re
import execjs

session = requests.session()
maps = lambda x: x[0] if x else x

def woff_font(font_url):
    '''获取字体真实对应关系'''
    newmap = {}
    resp = session.get(font_url)  # 请求字体链接
    woff_data = BytesIO(resp.content)  # 保存字体数据
    font = TTFont(woff_data)  # 读取woff数据
    glyf = font['glyf']  # 获取请求到的字体形状
    # 建立基础的字体和字体形状的对应关系
    base_font_map = {
        0: glyf['zero'],
        1: glyf['four'],
        2: glyf['five'],
        3: glyf['three'],
        4: glyf['seven'],
        5: glyf['one'],
        6: glyf['two'],
        7: glyf['six'],
        8: glyf['nine'],
        9: glyf['eight']
    }
    code_name_map = font.getBestCmap()  # 获取请求到的字体code和name的对应关系
    font.close()
    for code, name in code_name_map.items():
        codestr = str(code - 48)  # 根据分析结果需要减去48
        current_shape = glyf[name]  # 根据name获取字体形状
        for number, shape in base_font_map.items():  # 遍历基础字体形状对应关系
            if shape == current_shape:  # 判断，如果两个字体形状相等
                newmap[codestr] = str(number)  # 将字体编码和字体添加到字典
    return newmap


def font_file(r):
    # 从网页里面获取字体
    font_url = re.search(r"url\('(.*\.woff)'\)", r).group(1)
    return font_url


def get_index():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36"
    }
    url = 'https://www.renrenche.com/bj/ershouche/p2/?&plog_id=838083390d4b077a45852d11065f60ad'
    complete_cookie = {}
    # 第一次不带参数访问首页，获取 acw_tc 和 acw_sc__v2
    response = requests.get(url=url, headers=headers)
    complete_cookie.update(response.cookies.get_dict())
    arg1 = re.findall("arg1='(.*?)'", response.text)[0]
    with open('demo.js', 'r', encoding='utf-8') as f:
        acw_sc_v2_js = f.read()
    acw_sc__v2 = execjs.compile(acw_sc_v2_js).call('get_cookie', arg1)
    complete_cookie.update({"acw_sc__v2": acw_sc__v2})
    # 第二次访问首页，获取其他 cookies
    response2 = requests.get(url=url, headers=headers, cookies=complete_cookie)
    font_url = font_file(response2.text)
    print(font_url)
    html = etree.HTML(response2.text)
    items = html.xpath('//ul[@class="row-fluid list-row js-car-list"]/li')
    for i in items:
        title = maps(i.xpath('.//h3/text()'))
        font = woff_font(font_url)
        trans_title = "".join([i if not i.isdigit() else font[i] for i in title])  # 替换错误字体，获取真实标题
        print(trans_title)


get_index()

```

Cookie逆向的流程与上一节最后一个案例中反爬的方式**完全一致**。

要注意，多次请求可能会导致IP被检测，需要过一个滑块验证码，这个在后面对应的章节会详细讲解。







