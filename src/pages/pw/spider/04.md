---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 获取网站信息

本节主要讲解用的更多的`request`模块，`httpx`模块略讲。

## httpx模块

`httpx`是Python新一代的网络请求库，它包含以下特点

- 基于Python3的功能齐全的`http`请求模块
- 既能发送同步请求，也**能发送异步请求**
- 支持HTTP/1.1和HTTP/2
- 能够直接向WSGI应用程序或者ASGI应用程序发送请求

`httpx`模块使用前需要安装：

```python
pip3 install httpx
```

### 基本使用

`http.请求名称(url, *kwargs)`或`http.request(请求名称, url, *kwargs)`可以对指定`url`发送对应请求，

直接打印会返回请求结果（状态码 ），尝试一下：

```python
import httpx

r1 = httpx.get('https://httpbin.org/get')
print(f'r1: {r1}')

r2 = httpx.post('https://httpbin.org/post', data={'key':'value'})
print(f'r2: {r2}')

r3 = httpx.put('https://httpbin.org/put', data={'key':'value'})
print(f'r3: {r3}')

r4 = httpx.delete('https://httpbin.org/delete')
print(f'r4: {r4}')

r5 = httpx.head('https://httpbin.org/get')
print(f'r5: {r5}')

r6 = httpx.options('https://httpbin.org/get')
print(f'r6: {r6}')
```

打印结果：

```python
r1: <Response [200 OK]>
r2: <Response [200 OK]>
r3: <Response [200 OK]>
r4: <Response [200 OK]>
r5: <Response [200 OK]>
r6: <Response [200 OK]>
```

返回结果的`text`属性为相应内容，`httpx`会自动处理将响应内容解码为Unicode文本，可通过修改`r.encoding`属性修改编码：

```xml
<!doctype html><html lang="zh" dir="ltr"><head><meta name="theme-color" ...
```

响应内容也可以以字节形式显示，通过`content`属性即可：

```python
r = httpx.get('https://cn.bing.com/')
print(r.content)
```

打印结果：

```xml
b'<!doctype html><html lang="zh" dir="ltr"><head><meta name="theme-color" ...
```

也可以自定义请求头，通过关键字形参headers传递；参数可以通过关键字实参`params`进行传递：

```python
headers = {'user-agent': 'my-app/1.0.0'}
params = {'key1': 'value1', 'key2': 'value2'}
url = 'https://httpbin.org/get'
r = httpx.get(url, headers=headers, params=params)
print(r.text)
```

### httpx请求案例

```python
import httpx
import os

class Httpx_Req(object):
    def __init__(self):
        # 创造请求头
        self.headers = {
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.61 Safari/537.36'
            }
	
    # 保存爬虫任务
    def get_url_list(self):
        # 任务（需要爬取的url ）列表
        url_list = ['https://ts1.cn.mm.bing.net/th/id/R-C.ff5328fff6b3f981982436ba9d401ede?rik=yI9vC3TlhekslQ&riu=http%3a%2f%2fpic.baike.soso.com%2fp%2f20121002%2f20121002171749-795404783.jpg&ehk=ZS%2f%2fwXvfv1VGKM0OzwO2YTQBZxnIxNtp6zekHirBXP8%3d&risl=&pid=ImgRaw&r=0',
                    'https://ts1.cn.mm.bing.net/th/id/R-C.cd4d1907b4bf2fdef5b8ac246b20c6fa?rik=W8PYNxfYr82YQA&riu=http%3a%2f%2fi.gtimg.cn%2fqqlive%2fimg%2fjpgcache%2ffiles%2fqqvideo%2f9%2f9wcrz7hl4qasz4m.jpg&ehk=EE1F1DIjerWCgoAht40kgy19qgYTPZf82L4xEvJoLfk%3d&risl=&pid=ImgRaw&r=0',
                    'https://ts1.cn.mm.bing.net/th/id/R-C.167efdd588c795082eca759fc14d0787?rik=Jc5NusoDv9Z2aw&riu=http%3a%2f%2fd.hiphotos.baidu.com%2fzhidao%2fpic%2fitem%2ff31fbe096b63f62498a45fc18644ebf81b4ca373.jpg&ehk=Ona7eUyLsnKcFqaZda%2fOCCOCWi6USZgL1V66q08zaaQ%3d&risl=&pid=ImgRaw&r=0',
                    ]
        return url_list
	
    # 保存信息模块
    def save_image(self,filename,img):
        with open(filename, 'wb') as f:
            f.write(img.content)
        print('图片提取成功')

    # 主程序
    def run(self):
        url_list = self.get_url_list()
        for index,u in enumerate(url_list):
            file_name = './image/{}.jpg'.format(index)
            # 发送请求，等价于httpx.get(u, headers=self.headers)
            data = httpx.request('get', u, headers=self.headers) 
            self.save_image(file_name,data)

if __name__ == '__main__':
    r = Httpx_Req()
    if os.path.exists("./image") is False:		# 判断保存路径是否存在，不存在则创建
        os.mkdir('./image')
    r.run()										# 启动主程序

```

## requests 模块

`requests`模块比`httpx`更为强大。有了它，Cookies、登录验证、代理设置等操作都变得极为简单。

`requests`模块使用前需要安装：

```python
pip install requests
```

### 基本使用

`requests`中的`get`方法可以以 `GET` 方式请求网页，实例：

```python
import requests  

r = requests.get('https://www.baidu.com/')  
print(type(r))  
print(r.status_code)  
print(type(r.text))  
print(r.text)  
print(r.cookies)
```

其他种类的请求也相似：

```python
r = requests.post('http://httpbin.org/post')  
r = requests.put('http://httpbin.org/put')  
r = requests.delete('http://httpbin.org/delete')  
r = requests.head('http://httpbin.org/get')  
r = requests.options('http://httpbin.org/get')
```

#### GET请求示例

先以请求`json`文本为例：

```python
import requests  

data = {  
    'name': 'germey',  
    'age': 22  
}  
r = requests.get("http://httpbin.org/get", params=data)  
print(r.text)
```

也可抓取二进制数据，以图片为例：

```python
import requests

r = requests.get("https://gss0.baidu.com/7Po3dSag_xI4khGko9WTAnF6hhy/zhidao/pic/item/3b292df5e0fe9925538aff9c33a85edf8cb1717c.jpg")

print(r.text)		# 乱码数据

print(r.content)	# 字节码数据
```

尽管`requests`中封装了请求头，但其中一定含有请求头字段`User-Agent`，且其值固定为`python urllib/3.3.0`。

在一些情况下，一些服务器会将带有该字段值的请求头捕获，使其不能正常请求。

这种情况需要我们额外添加请求头，使用关键字实参`headers`即可，示例：

```python
import requests

r = requests.get("https://blog.csdn.net/")      # 直接请求

print(r.text)   # 请求体中无数据
```

如果加上包含`User-Agent`信息的请求头，即可正常得到响应：

```python
import requests
# 构造请求头
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}

r = requests.get("https://blog.csdn.net/", headers=headers)

print(r.text)
```

#### POST请求示例

使用 `requests` 实现 `POST` 请求同样非常简单，示例如下：

```python
import requests

data = {'name': 'germey', 'age': '22'}
r = requests.post("http://httpbin.org/post", data=data)
print(r.text)
```

案例：爬取巨潮网络数据，点击资讯选择公开信息中的数据，地址为`http://www.cninfo.com.cn/data20/ints/statistics`：

```python
import requests
url= 'http://www.cninfo.com.cn/data20/ints/statistics'
res = requests.post(url)
print(res.text)
```

#### 获取其它响应内容

在上面的实例中，我们使用 `text` / `content` 获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies 等，示例：

```python
import requests
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'
}
r = requests.get('http://www.jianshu.com', headers=headers)
print(type(r.status_code), r.status_code)
print(type(r.headers), r.headers)
print(type(r.cookies), r.cookies)
print(type(r.url), r.url)
print(type(r.history), r.history)
```

状态码常用来判断**请求是否成功**，而 `requests` 还提供了一个内置的状态码查询对象 `requests.codes`来解决这个问题：

下面列出了返回码和相应的查询条件：

```python
# 信息性状态码  
100: ('continue',),  
101: ('switching_protocols',),  
102: ('processing',),  
103: ('checkpoint',),  
122: ('uri_too_long', 'request_uri_too_long'),  

# 成功状态码  
200: ('ok', 'okay', 'all_ok', 'all_okay', 'all_good', '\\o/', '✓'),  
201: ('created',),  
202: ('accepted',),  
203: ('non_authoritative_info', 'non_authoritative_information'),  
204: ('no_content',),  
205: ('reset_content', 'reset'),  
206: ('partial_content', 'partial'),  
207: ('multi_status', 'multiple_status', 'multi_stati', 'multiple_stati'),  
208: ('already_reported',),  
226: ('im_used',),  

# 重定向状态码  
300: ('multiple_choices',),  
301: ('moved_permanently', 'moved', '\\o-'),  
302: ('found',),  
303: ('see_other', 'other'),  
304: ('not_modified',),  
305: ('use_proxy',),  
306: ('switch_proxy',),  
307: ('temporary_redirect', 'temporary_moved', 'temporary'),  
308: ('permanent_redirect',  
      'resume_incomplete', 'resume',), # These 2 to be removed in 3.0  

# 客户端错误状态码  
400: ('bad_request', 'bad'),  
401: ('unauthorized',),  
402: ('payment_required', 'payment'),  
403: ('forbidden',),  
404: ('not_found', '-o-'),  
405: ('method_not_allowed', 'not_allowed'),  
406: ('not_acceptable',),  
407: ('proxy_authentication_required', 'proxy_auth', 'proxy_authentication'),  
408: ('request_timeout', 'timeout'),  
409: ('conflict',),  
410: ('gone',),  
411: ('length_required',),  
412: ('precondition_failed', 'precondition'),  
413: ('request_entity_too_large',),  
414: ('request_uri_too_large',),  
415: ('unsupported_media_type', 'unsupported_media', 'media_type'),  
416: ('requested_range_not_satisfiable', 'requested_range', 'range_not_satisfiable'),  
417: ('expectation_failed',),  
418: ('im_a_teapot', 'teapot', 'i_am_a_teapot'),  
421: ('misdirected_request',),  
422: ('unprocessable_entity', 'unprocessable'),  
423: ('locked',),  
424: ('failed_dependency', 'dependency'),  
425: ('unordered_collection', 'unordered'),  
426: ('upgrade_required', 'upgrade'),  
428: ('precondition_required', 'precondition'),  
429: ('too_many_requests', 'too_many'),  
431: ('header_fields_too_large', 'fields_too_large'),  
444: ('no_response', 'none'),  
449: ('retry_with', 'retry'),  
450: ('blocked_by_windows_parental_controls', 'parental_controls'),  
451: ('unavailable_for_legal_reasons', 'legal_reasons'),  
499: ('client_closed_request',),  

# 服务端错误状态码  
500: ('internal_server_error', 'server_error', '/o\\', '✗'),  
501: ('not_implemented',),  
502: ('bad_gateway',),  
503: ('service_unavailable', 'unavailable'),  
504: ('gateway_timeout',),  
505: ('http_version_not_supported', 'http_version'),  
506: ('variant_also_negotiates',),  
507: ('insufficient_storage',),  
509: ('bandwidth_limit_exceeded', 'bandwidth'),  
510: ('not_extended',),  
511: ('network_authentication_required', 'network_auth', 'network_authentication')
```

示例如下：

```python
import requests
headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'}
r = requests.get('http://www.jianshu.com',headers=headers)
# 若状态码对应值为'ok'(即200)，则打印请求成功
exit() if not r.status_code == requests.codes.ok else print('Request Successfully')
```

### 高级用法

#### 代理添加

当请求次数过多时，本地`IP`可能会被服务端封禁。我们可通过关键字形参`proxies`来指定本机的代理`IP`，例如：

```python
# 构造代理ip对象
proxy = {
    'http': 'http://183.162.171.78:4216',
}
# 返回当前IP
res = requests.get('http://httpbin.org/ip', proxies=proxy)
print(res.text)
```

国内有很多好用的代理池，比如快代理等，可以每次在代理池中随机选一个代理IP进行请求。

也可以自己写一个IP代理池，处理IP被封禁的情况，难度较大（illegal ）。

#### 关闭警告

在爬虫过程中，若不作处理，终端中会显示许多警告信息，如请求失败、网络异常等。可以使用以下代码关闭这些警告：

+ Python2版本写法：

  ```python
  from requests.packages import urllib3
  urllib3.disable_warnings()
  ```

  若只需忽略特定类型的警告（推荐 ），可在`requests.packages.urllib3.exceptions`模块中引入警告类型，再作为参数填入：

  ```python
  from requests.packages.urllib3.exceptions import InsecureRequestWarning
  # 仅禁用安全请求警告InsecureRequestWarning
  requests.packages.urllib3.disable_warnings(InsecureRequestWarning)
  ```

+ Python3版本写法：

  ```python
  import urllib3
  urllib3.disable_warnings()
  ```

  忽略一部分警告的写法也相似，在`urllib3.exceptions`库中导入：

  ```python
  import urllib3
  urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
  ```

### 初级爬虫

在上一节的最后，我们介绍了爬虫的基本流程，如下图所示：

![image-20210727222631254](https://images.maiquer.tech/images/wx/image-20210727222631254.png)

以爬取`36氪`网站 (`https://36kr.com/information/technology`) 的数据：

```python
# https://36kr.com/information/technology
import requests
# 用于数据解析
from lxml import etree

def main():
    # 1. 定义页面URL和解析规则
    crawl_urls = [
        'https://36kr.com/p/1328468833360133',
        'https://36kr.com/p/1328528129988866',
        'https://36kr.com/p/1328512085344642'
    ]
    # 这个是xpath数据解析规则，在解析HTML时使用，下节会具体讲
    parse_rule = "//h1[contains(@class,'article-title margin-bottom-20 common-width')]/text()"

    for url in crawl_urls:
        # 2. 发起HTTP请求
        response = requests.get(url)
        # 3. 解析HTML，下节会讲
        result = etree.HTML(response.text).xpath(parse_rule)[0]
        # 4. 保存结果 (这里暂时仅打印，下下节会讲数据存储)
        print(result)

if __name__ == '__main__':
    main()
```

### 全站采集

爬虫程序与其它程序一样，也有许多需要复用的模块，我们需要将这些模块进行**封装**。下面就是基于`requests`请求库的封装样例：

（可以拿去直接用 ）首先创建`utils`文件夹，写一个`base`类供其他程序调用（作为**基类**对其继承 ）.

越是与用户操作的方式接近，被IP的风险越小，所以在以下代码中使用了随机生成的请求头和请求后的随机等待时间。

为了提高爬虫的成功率，采用了`retrying`模块中的装饰器`retry`，若执行阻塞或失败，将间隔指定时间执行指定次数，直至执行完成。

```python
# utils/base.py

import random
import time
import requests
import urllib3
from retrying import retry
from urllib3.exceptions import InsecureRequestWarning

# 禁用安全请求警告
urllib3.disable_warnings(InsecureRequestWarning)

# 每次随机生成不同且合法的User-Agent请求头字段，降低被封风险。
# 使用FakeChromeUA.get_ua()方法获取随机User-Agent
class FakeChromeUA:
    first_num = random.randint(55, 62)
    third_num = random.randint(0, 3200)
    fourth_num = random.randint(0, 140)
    os_type = [
        '(Windows NT 6.1; WOW64)', '(Windows NT 10.0; WOW64)', '(X11; Linux x86_64)',
        '(Macintosh; Intel Mac OS X 10_12_6)'
    ]
    # 伪造chrome浏览器版本号，合法即可
    chrome_version = 'Chrome/{}.0.{}.{}'.format(first_num, third_num, fourth_num)

    @classmethod
    def get_ua(cls):
        return ' '.join(
            ['Mozilla/5.0', random.choice(cls.os_type), 'AppleWebKit/537.36', '(KHTML, like Gecko)', cls.chrome_version,
             'Safari/537.36'])


# 爬虫基类主程序，继承FakeChromeUA类，意味着可以使用基类中的get_ua()方法
class Spiders(FakeChromeUA):
    # 用于保存爬虫任务（url ）列表
    urls = []

    # retry装饰器会指定程序执行的次数(stop_max_attempt_number)，执行等待的时间(wait_fixed)
    @retry(stop_max_attempt_number=3, wait_fixed=2000)
    def fetch(self, url, param=None, headers=None):
        try:
            if not headers:     # 若未传递headers，创建headers
                headers = {'user-agent': self.get_ua()}
            else:               # 否则添加User-Agent字段
                headers['user-agent'] = self.get_ua()
            # 每次请求前，隔一段时间再进行请求
            self.wait_some_time()
            # 发起请求
            response = requests.get(url, params=param, headers=headers)
            # 若请求成功，返回请求结果
            if response.status_code == 200:
                response.encoding = 'utf-8'
                return response
        # 若连接失败，捕获异常，什么也不做
        except requests.ConnectionError:
            return

    # 随机等待一些时间
    def wait_some_time(self):
        time.sleep(random.randint(100, 300) / 1000)
```

下面我们就运用上面的例子进行**整站爬虫**，我们依旧采集36氪`https://36kr.com/information/technology`上的数据，但这次我们不仅需要将网站的源代码爬取下来，还需要解析其中的数据并存入`mongoDB`数据库（这两步在后两节会详细讲解 ），即完整的爬虫流程。

其中`pymongo`需要进行`pip3`安装

```python
import requests
from lxml import etree
from queue import Queue
from utils.base import Spiders
from pymongo import MongoClient
from urllib.parse import urljoin

# 过滤器函数，当x为假时返回None，否则返回x[0]
flt = lambda x: x[0] if x else None

# 继承Spiders基类，可以使用基类中的基础方法
class Crawl(Spiders):
    # 基地址
    base_url = 'https://36kr.com/'
    # 种子URL
    start_url = 'https://36kr.com/information/technology'
    # 解析规则
    rules = {
        # 文章列表
        'list_urls': '//div[@class="article-item-pic-wrapper"]/a/@href',
        # 详情页数据
        'detail_urls': '//div[@class="common-width margin-bottom-20"]//text()',
        # 标题
        'title': '//h1[@class="article-title margin-bottom-20 common-width"]/text()',
    }
    # 定义队列，用于保存任务
    list_queue = Queue()

    # 采集首页中列表页的URL
    def crawl(self, url):
        # 调用基类中的fetch方法，获取响应内容
        response = self.fetch(url)
        # 对响应内容进行xpath解析，提取列表页的URL
        list_urls = etree.HTML(response.text).xpath(self.rules['list_urls'])
        for list_url in list_urls:
            # urljoin用于拼接完整的url，拼接后将url放入任务队列中
            self.list_queue.put(urljoin(self.base_url, list_url))

    # 采集列表页信息
    def list_loop(self):
        while True:
            # 从任务队列中取出一个url
            list_url = self.list_queue.get()
            # 采集详情页信息，另做封装
            self.crawl_detail(list_url)
            # 如果队列为空 退出程序
            if self.list_queue.empty():
                break

    # 采集详情页信息
    def crawl_detail(self, url):
        # 请求
        response = self.fetch(url)
        # 按规则解析数据（标题和内容 ）
        html = etree.HTML(response.text)
        title = flt(html.xpath(self.rules['title']))
        content = html.xpath(self.rules['detail_urls'])
        # 将数据封装为字典
        data = {
            'content': content,
            'title': title
        }
        # 调用保存模块
        self.save_mongo(data)

    # 将数据保存到MongoDB
    def save_mongo(self, data):
        client = MongoClient()          # 建立连接
        # 指定数据库
        col = client['python']['hh']
        # 若数据格式正确则保存数据，否则打印错误信息
        if isinstance(data, dict):
            res = col.insert_one(data)
            return res
        else:
            return '单条数据必须是这种格式：{"name":"age"}，你传入的是%s' % type(data)

    # 主程序，调用方法即可
    def main(self):
        self.crawl(self.start_url)
        self.list_loop()


if __name__ == '__main__':
    s = Crawl()
    s.main()
```

## requests-cache

在做爬虫的时候，我们往往可能这些情况：

- 网站比较复杂，会碰到很多重复请求。
- 有时候爬虫意外中断了，但我们没有保存爬取状态，再次运行就需要重新爬取。

`requests-cache`模块是`requests`的拓展模块。当`requests`重复向同一个URL发送请求的时候，`requests-cache`会判断当前请求是否已产生缓存，若已有缓存，则从缓存里读取数据作为响应内容；若没有缓存，则向网站服务器发送请求，并将得到的响应内容写入相应的数据库里。

`requests-cache`模块使用前需要安装：

```python
pip install requests-cache
```

### 速度对比

我们先用`requests`请求库对`http://httpbin.org/delay/1`进行请求，且请求间保持每次的参数（`request.Session()` ）:

```python
import requests
import time

start = time.time()
session = requests.Session()
for i in range(10):
    session.get('http://httpbin.org/delay/1')
    print(f'Finished {i + 1} requests')
end = time.time()
print('Cost time', end - start)
```

可见，最后用时约为5秒钟，我们再使用`request-cache`完成同样的操作。需要先声明缓存块名称，格式为：

```python
session = requests_cache.CachedSession('demo_cache')
```

再通过`session.get()`即可发起请求：

```python
import requests_cache
import time

start = time.time()
session = requests_cache.CachedSession('demo_cache')

for i in range(10):
    session.get('http://httpbin.org/delay/1')
    print(f'Finished {i + 1} requests')
end = time.time()
print('Cost time', end - start)
```

最后用时约为3秒钟左右。

### 基本使用

但是，刚才我们在写的时候把`requests`的`session`对象直接替换了。

有没有别的写法呢？比如我不影响当前代码，只在代码前面加几行初始化代码就完成 `requests-cache` 的配置呢？

当然也是可以的，直接调用 `requests-cache` 库的 `install_cache`方法即可，其他的 `requests` 的 `Session` 照常使用：

```python
import time
import requests
import requests_cache

requests_cache.install_cache('demo_cache')

start = time.time()
session = requests.Session()
for i in range(10):
    session.get('http://httpbin.org/delay/1')
    print(f'Finished {i + 1} requests')
end = time.time()
print('Cost time', end - start)
```

`requests-cache` 默认使用了 `SQLite` 作为缓存对象，缓存对象也可以被替换成本地文件等其他存储载体。

例如把缓存对象换成本地文件，那可以这么做：

```python
requests_cache.install_cache('demo_cache', backend='filesystem')
```

如果不想生产文件，可以指定`use_cache_dir`参数为`True`，表示不产生缓存文件，默认为`False`：

```python
requests_cache.install_cache('demo_cache', backend='filesystem', use_cache_dir=True)
```

另外除了文件系统，`requests-cache`也支持其他的后端存储载体，比如`Redis`、`MongoDB`、`GridFS` 甚至内存，但也需要对应的依赖库支持，具体可以参见下表：

| 后端存储载体 | 类 名           | 别 名          | 依 赖 库 |
| :----------- | :-------------- | :------------- | :------- |
| SQLite       | `SQLiteCache`   | `'sqlite'`     |          |
| Redis        | `RedisCache`    | `'redis'`      | redis-py |
| MongoDB      | `MongoCache`    | `'mongodb'`    | pymongo  |
| GridFS       | `GridFSCache`   | `'gridfs'`     | pymongo  |
| DynamoDB     | `DynamoDbCache` | `'dynamodb'`   | boto3    |
| Filesystem   | `FileCache`     | `'filesystem'` |          |
| Memory       | `BaseCache`     | `'memory'`     |          |

比如使用`Redis`就可以改写如下：

```python
backend = requests_cache.RedisCache(host='localhost', port=6379)
requests_cache.install_cache('demo_cache', backend=backend)
```

更多详细配置可以参考官方文档：https://requests-cache.readthedocs.io/en/stable/user_guide/backends.html#backends

当然，我们有时候也想指定有些请求不缓存，比如只缓存`POST`请求，不缓存`GET` 请求，那可以在`install_cache()`方法中传入`allowable`参数使用来配置。该参数为一个列表，传入允许的请求类型字符串，示例：

```python
import time
import requests
import requests_cache

requests_cache.install_cache('demo_cache2', allowable_methods=['POST'])

start = time.time()
session = requests.Session()
for i in range(10):
    session.get('http://httpbin.org/delay/1')
    print(f'Finished {i + 1} requests')
end = time.time()
print('Cost time for get', end - start)
start = time.time()

for i in range(10):
    session.post('http://httpbin.org/delay/1')
    print(f'Finished {i + 1} requests')
end = time.time()
print('Cost time for post', end - start)
```

当然我们还可以匹配 URL，比如针对哪种 URL 缓存多久，可以添加`urls_expire_after=urls_expire_after`参数：

```python
urls_expire_after = {'*.site_1.com': 30, 'site_2.com/static': -1}
requests_cache.install_cache('demo_cache2', urls_expire_after=urls_expire_after)
```

好了，到现在为止，一些基本配置、过期时间配置、后端配置、过滤器配置等基本常见的用法就介绍到这里啦，更多详细的用法大家可以参考官方文档：https://requests-cache.readthedocs.io/en/stable/user_guide.html。



