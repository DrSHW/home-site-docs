---
description: Docs intro
layout: ../../../layouts/MainLayout.astro
---

# 线性回归模型

## 简要介绍

+ 回归是统计学的一个重要概念，是一种**有监督**算法
+ 回归算法是一种比较常用的机器学习算法，用于构建一个模型来做特征向量到标签的映射。在算法的学习过程中，试图寻找一个模型，最大程度你和训练数据。
+ 回归算法在使用时，接收一个`n`维特征向量，输出一个**连续**的数据值。
+ 线性是指量与量之间按照**比例**成直线关系，在数学上可理解对各变量求一阶导数结果为常数。

## 单变量线性回归模型

单变量是指样本的特征（影响结果的因素 ）唯一，结果为`y = ax + b`的形式。

### 应用案例

假设在同一区域内，房子的**租赁价格**与**尺寸**成正相关。如果我们有很多组**数据**，我们是否可以将两者的大致关系计算出来？

如下图，红色的叉即代表样本点，线性回归算法就是要找到一条**直线**，即下图那条蓝色的直线，使其与所有样本点契合的足够好。

找到了直线，也就是找到了关系式，即**模型**。

![](https://images.maiquer.tech/images/wx/image-20220719202553497.png)

### 算法实现

我们可以通过不断进行**梯度下降**（算法 ）、**线性回归**（模型 ）、**性能评价**（代价函数 ）操作来得到这条直线。

先指定一个随机值，对其进行如上操作，效果如下图：

<video src="https://images.maiquer.tech/images/wx/process.mp4"></video>

我们可以指定一个迭代次数，迭代完毕后即可认为回归完成。

#### 数据抽象

对具体问题进行数学抽象，一般分为数据集、模型、参数（不确定的那些数 ）三部分：

+ 数据集为：![](https://images.maiquer.tech/images/wx/image-20220719212426796.png)

  其中`m`为数据总数

+ 目标线性模型：![](https://images.maiquer.tech/images/wx/image-20220719205256985.png)

  即直线方程

+ 参数：![](https://images.maiquer.tech/images/wx/image-20220719205536674.png)

我们只需想办法确定两个参数即可。

#### 代价函数——最小二乘法

**代价函数**是评判模型性能（效果 ）的指标，通常是函数表达式。

比如说如下两条直线，红线“考虑”到了所有样本点，因此预测效果的要比蓝线的好：

![](https://images.maiquer.tech/images/wx/image-20220719210656962.png)

在此情景中，我们可以通过**最小二乘法**来计算代价函数。

**最小二乘法**（又称最小平方法 ）是一种数学优化技术。它由两部分组成：

1. 计算所有样本**误差**的**平均**（代价函数 ）

2. 使用最优化方法寻找数据的最佳函数匹配（抽象的 ）

最小二乘法是抽象的，具体的最优化方法有很多，比如**正规方程法**、**梯度下降法**、牛顿法、拟牛顿法等等。

在这里，误差即为样本点偏离直线的距离，如下图，即为样本点到直线的垂直距离：

![image-20220719213021928](https://images.drshw.tech/images/notes/image-20220719213021928.png)

由于代价函数为所有误差的平均值，故此处求代价函数的**最小值**即可。

于是，我们可以推导代价函数：

![](https://images.maiquer.tech/images/wx/image-20220719212745672.png)

使用**梯度下降**算法可以求取代价函数最小值。

#### 梯度下降（Gradient Descent ）

梯度下降是一个用来求函数**最小值**的算法，我们将使用梯度下降算法来求出代价函数J`(θ0, θ1)` 的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合`(θ0, θ1)`，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。

![](https://images.maiquer.tech/images/wx/image-20220719215819669.png)

代价函数为二元函数，它的图像类似上图所示。可以发现当图像到达最低点时，其每个方向上的偏导数值均为0，即：

<img src="https://images.maiquer.tech/images/wx/image-20220719222725186.png" style="zoom:50%;" />

梯度下降算法思想：将选取的值`(θ0, θ1)`分别带入偏导结果，此时结果为**变化率**。

我们取一个很小的值，让各个参数减去该值与变化率的乘积，即可让其逼近偏导为0的点，不断重复这样的操作，就可以计算出最小值。

很小的值即为是**学习率**（learning rate ），用`α`表示，通常是一个大于0的很小的经验值，它决定沿着能让代价函数下降的程度有多大。

每一次都同时让**所有的参数**减去学习速率乘以代价函数的导数即可，直到代价函数趋近于0。

由迭代过程可以看出，梯度下降只能用来求取**局部最优解**。对于函数有多个极小值点的情况，梯度下降有可能会陷于某个局部最优解中，而未找到全局最优解。但在单变量线性回归中，由于代价函数只会有一个极小值点，故梯度下降找到的解一定是全局最优解。

对于`α`的选取：

+ 如果`α`太小，梯度下降就会变慢。
+ 如果`α`太大，梯度下降法可能会越过最低点。甚至可能无法收敛，或者偏离。

于是，我们可以得到如下算法流程：

![](https://images.maiquer.tech/images/wx/image-20220719230859090.png)

其中![](https://images.maiquer.tech/images/wx/image-20220719232432776.png)

于是，我们可以得到最终的数据集和模型：

![](https://images.maiquer.tech/images/wx/image-20220719233853826.png)

接下来，我们就可以使用代码进行数据处理：

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.array([4, 3, 3, 4, 2, 2, 0, 1, 2, 5, 1, 2, 5, 1, 3])         # 样本值
y = np.array([8, 6, 6, 7, 4, 4, 2, 4, 5, 9, 3, 4, 8, 3, 6])         # 样本值
m = len(x)  # 获取样本的数量
# x增加一列1
x = np.concatenate((np.ones([m, 1]), x.reshape(m, 1)), axis=1)
# 为了后续维度对应，y也做维度变化
y = y.reshape(15, 1)  # 列向量
# 初始化参数
theta = np.zeros([2, 1])  # 随机创建参数theta，这里取0
cost = np.zeros([1, 1])  # 初始化代价函数序列J(theta)
y_hat = np.zeros([15, 1])  # 初始化预测值序列h(x)
# 梯度下降法
alpha = 0.01  # 学习率
iterations = 3  # 指定迭代次数
# 循环求解
for i in range(iterations):
    y_hat = x.dot(theta)  # 计算预测值h(x)
    cost = (y_hat - y).T.dot(y_hat - y) / (2 * m)  # 计算代价函数J(theta)
    delta_theta = x.T.dot(x.dot(theta) - y) / m  # 求偏导数值（梯度 ）
    theta = theta - alpha * delta_theta  # 更新theta
print(f'两参数的值为：{theta}')
print(f'代价函数的值为：{cost}')
# 用蓝实线绘制预测图线
plt.plot(x[:, 1], y_hat, 'b')  # 画图
# 用红叉绘制样本点
plt.plot(x[:, 1], y, 'ro')
plt.show()
```

运行后输出：

```python
'''
两参数的值为：[[1.84453782]
 [1.35084034]]
代价函数的值为：[[0.10091036]]
'''
```

图像如下：

![](https://images.maiquer.tech/images/wx/image-20220720175537948.png)

